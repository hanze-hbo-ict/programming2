{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Programming 2 \u00b6 These pages provide the materials, information, and assignments for the module Programming 2 for the Master Digital Science for Life Science. As its name suggests, this module is the follow-up of Programming 1 and it builds on the tools and techniques that were used in that first module. It aims to make students competent in designing parallel solutions for computational problems that aren\u2019t adequately solvable by a single computer. In order to do so, we first refresh the topics of Programming 1 , we look some more into object life-cycle and multiple class interaction. After this, we look into the Divide and Conquer and Map Reduce algorithms, which provides us the mindset of separating our unit of work into several sub-problems, that can be easily distributed over several machines. In the second part of this module, we will actually perform this distribution. We will talk at some length about parallelism and network operations. After that, we will use systems such as Spark of Dask to run several analyses of complex data sets. We end the module by looking at best practices from software engineering to make our own code base better. Schedule and links \u00b6 Part 1 weeknumber subject(s) 1 Refresh UML; SOLID; OO en design patterns 2 Classes and Objects; Constructors and Destructors; Object Lifecycle; Dunders 3 Multiple Class Interaction; modules 4 List Comprehensions; Generators; map-reduce 5 Unit of Work; Divide and Conquer algoritms Part 2 weeknumber subject(s) 1 Network Operations; POSIX stack 2 Parallellisation 3 Slurm and Dask 1 4 Slurm and Dask 2 5 MLOps 6 SE4ML 7 wrap up Assignment \u00b6 During the course, students will work individually on (more or less) weekly assignments. Every week, students are given time to work on these assignments and will give peer feedback on each other's work. Students can improve their elaboration on basis of this feedback. At the end of the term, the collection of all these elaborations will form a portfolio which will be graded. Should the portfolio be considered insufficient, specific repair assignments will be given.","title":"Programming 2"},{"location":"index.html#programming-2","text":"These pages provide the materials, information, and assignments for the module Programming 2 for the Master Digital Science for Life Science. As its name suggests, this module is the follow-up of Programming 1 and it builds on the tools and techniques that were used in that first module. It aims to make students competent in designing parallel solutions for computational problems that aren\u2019t adequately solvable by a single computer. In order to do so, we first refresh the topics of Programming 1 , we look some more into object life-cycle and multiple class interaction. After this, we look into the Divide and Conquer and Map Reduce algorithms, which provides us the mindset of separating our unit of work into several sub-problems, that can be easily distributed over several machines. In the second part of this module, we will actually perform this distribution. We will talk at some length about parallelism and network operations. After that, we will use systems such as Spark of Dask to run several analyses of complex data sets. We end the module by looking at best practices from software engineering to make our own code base better.","title":"Programming 2"},{"location":"index.html#schedule-and-links","text":"Part 1 weeknumber subject(s) 1 Refresh UML; SOLID; OO en design patterns 2 Classes and Objects; Constructors and Destructors; Object Lifecycle; Dunders 3 Multiple Class Interaction; modules 4 List Comprehensions; Generators; map-reduce 5 Unit of Work; Divide and Conquer algoritms Part 2 weeknumber subject(s) 1 Network Operations; POSIX stack 2 Parallellisation 3 Slurm and Dask 1 4 Slurm and Dask 2 5 MLOps 6 SE4ML 7 wrap up","title":"Schedule and links"},{"location":"index.html#assignment","text":"During the course, students will work individually on (more or less) weekly assignments. Every week, students are given time to work on these assignments and will give peer feedback on each other's work. Students can improve their elaboration on basis of this feedback. At the end of the term, the collection of all these elaborations will form a portfolio which will be graded. Should the portfolio be considered insufficient, specific repair assignments will be given.","title":"Assignment"},{"location":"week1.1.html","text":"Week 1.1: Static code analysis \u00b6 In this exercise we are going to look at the code delivered by a student. Download the code-basae using this link . Because of reasons of privacy and NDA's, we have changed the code somewhat, so if you see any strange stuff that's probably because of that. Static code analysis is meant to be done without running or even compiling the code. It can be done using techniques as fgrep , but also just by glancing over the code, opening files and see how they interact with each other. Different, very professional techniques exist for these kinds of jobs, but for this small code base doing stuff by hand will suffice. In this session, we will divide the group in five different sub-groups. Each group will work on the following assignments for about an hour and a half. After that, each sub-group will present one of the exercises in a small pitch: no PowerPoint or other presentation is required, just talk us through your findings. Exercise 1: The factory \u00b6 The application can parse the report in two different formats as a pdf file or as a text file. In the future, it could be possible that the hospitals share the information in another file format than the two that are currently implemented. To extend the functionality in the feature, a parent class HsmrParser was created which must be extended in the implementations of the specific parsers. At the moment, two different parsers are realised, but more can be added in the future. A factory was created to get an instance of the parser based on the parser types which are defined as constants in the file parserTypes.py file. What is a factory? Does the implementation of the factory method follow the Interface Segregation Principle ? Exercise 2: Single reponsibility \u00b6 The application uses the CCS classification to extract the information from the HSMR reports. The application uses a CSV file with the ccs index and the corresponding Dutch description. This information is provided by the CBS as a Microsoft Office Excel file. The necessary data was extracted and saved as a CSV file. Review the python files starting with Ccs. Are those files adhering to the single-responsibility principle: \"Every class should have only one responsibility\u201d? Exercise 3: The base classes \u00b6 In the code, several base classes are used. Can you find examples of the Liskov substitution principle : \"Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\" Explain your answer. Exercise 4. The local settings object \u00b6 A settings file was used inside the application to store the settings which can be changed over time or are user-specific. For instance, the path to a temp directory is saved inside the settings file because they will be different between users. Also, specific table names of the HSMR report, API URLs, and API headers are saved in this file because they can change and must be easily assessable. The Settings object is implemented as a Singleton object inside the application to prevent multiple instances of the same class. It also prevents multiple unnecessary parsing of the settings file and eventually different settings when the settings are manually changed during runtime of the program. Search for the Settings class. What makes this class a singleton object and is a singleton object SOLID? The hospital types codes are stored in a python module hospital_types.py . Is this a logical solution? Is there an alternative solution for these kinds of local settings and parameters? Please elaborate. Exercise 5. UML Class diagram \u00b6 Draw the class diagram for this program.","title":"Week 1.1: Static code analysis"},{"location":"week1.1.html#week-11-static-code-analysis","text":"In this exercise we are going to look at the code delivered by a student. Download the code-basae using this link . Because of reasons of privacy and NDA's, we have changed the code somewhat, so if you see any strange stuff that's probably because of that. Static code analysis is meant to be done without running or even compiling the code. It can be done using techniques as fgrep , but also just by glancing over the code, opening files and see how they interact with each other. Different, very professional techniques exist for these kinds of jobs, but for this small code base doing stuff by hand will suffice. In this session, we will divide the group in five different sub-groups. Each group will work on the following assignments for about an hour and a half. After that, each sub-group will present one of the exercises in a small pitch: no PowerPoint or other presentation is required, just talk us through your findings.","title":"Week 1.1: Static code analysis"},{"location":"week1.1.html#exercise-1-the-factory","text":"The application can parse the report in two different formats as a pdf file or as a text file. In the future, it could be possible that the hospitals share the information in another file format than the two that are currently implemented. To extend the functionality in the feature, a parent class HsmrParser was created which must be extended in the implementations of the specific parsers. At the moment, two different parsers are realised, but more can be added in the future. A factory was created to get an instance of the parser based on the parser types which are defined as constants in the file parserTypes.py file. What is a factory? Does the implementation of the factory method follow the Interface Segregation Principle ?","title":"Exercise 1: The factory"},{"location":"week1.1.html#exercise-2-single-reponsibility","text":"The application uses the CCS classification to extract the information from the HSMR reports. The application uses a CSV file with the ccs index and the corresponding Dutch description. This information is provided by the CBS as a Microsoft Office Excel file. The necessary data was extracted and saved as a CSV file. Review the python files starting with Ccs. Are those files adhering to the single-responsibility principle: \"Every class should have only one responsibility\u201d?","title":"Exercise 2: Single reponsibility"},{"location":"week1.1.html#exercise-3-the-base-classes","text":"In the code, several base classes are used. Can you find examples of the Liskov substitution principle : \"Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\" Explain your answer.","title":"Exercise 3: The base classes"},{"location":"week1.1.html#exercise-4-the-local-settings-object","text":"A settings file was used inside the application to store the settings which can be changed over time or are user-specific. For instance, the path to a temp directory is saved inside the settings file because they will be different between users. Also, specific table names of the HSMR report, API URLs, and API headers are saved in this file because they can change and must be easily assessable. The Settings object is implemented as a Singleton object inside the application to prevent multiple instances of the same class. It also prevents multiple unnecessary parsing of the settings file and eventually different settings when the settings are manually changed during runtime of the program. Search for the Settings class. What makes this class a singleton object and is a singleton object SOLID? The hospital types codes are stored in a python module hospital_types.py . Is this a logical solution? Is there an alternative solution for these kinds of local settings and parameters? Please elaborate.","title":"Exercise 4. The local settings object"},{"location":"week1.1.html#exercise-5-uml-class-diagram","text":"Draw the class diagram for this program.","title":"Exercise 5. UML Class diagram"},{"location":"week1.2.html","text":"Week 1.2: Classes and instances \u00b6 Introduction \u00b6 Photosynthesis is the process by which plants and certain algae convert light energy into chemical energy that can be stored and used to drive the organism's activities. Though different varieties of photosynthesis exist, the overall equation for the type that occurs in plants is as follows: \\(6CO_2 + 6H_2O -> C_6H_{12}O_6 + 6O_2 + energy\\) In this exercise, we are going to create a very simple model of this process. Assignment 1: the Atom class \u00b6 1a. Create a class Atom that is a representation of any atom in the periodic table. Make sure that when a concrete atom is instantiated, it is given its symbol, its atomic number and the number of neutrons in the core. Store those parameters in the created object. 1b. Create a method proton_number that returns the number of protons in the nucleus; make another method mass_number that returns the sum of protons and neutrons in the nucleus. Isotopes are types of atoms that have the same number of atomic number but a different number of neutrons in the core. So, e.g. 'normal' hydrogen has 1 proton and 1 neutron, but it also comes in the form of deuterium (1 proton and 2 neutrons) or even tritium (1 proton and 3 neutrons). 1c. Create a method isotope in the class Atom . When this method is called, the normal number of neutrons must be replaced by whatever number is provided to this method. 1d. We define an atom A to be less than another atom B if their proton number is the same (i.e. it is the same element) but the mass number of A is less than the mass number of B. Implement the methods that checks whether two isotopes of the same element are equal to each other, or less than or greater than each other. Raise an exception when the check is called with different types of elements. You can use the code below to test your implementation. protium = Atom ( 'H' , 1 , 1 ) deuterium = Atom ( 'H' , 1 , 2 ) oxygen = Atom ( 'O' , 8 , 8 ) tritium = Atom ( 'H' , 1 , 2 ) tritium . isotope ( 3 ) assert tritium . neutrons == 3 assert tritium . mass_number () == 4 assert protium < deuterium assert deuterium <= tritium assert tritium >= protium print ( oxygen > tritium ) # <-- this should raise an Exception Assignment 2: the Molecule class \u00b6 A molecule is a neutral group of two or more atoms. 2a. Create the class Molecule . When creating an instance of this class, a list of tuples of two values (a pair ) is given. The first element of this pair is the Atom-object, and the second element is the number of atoms of that type that is put into the molecule. Thus, the following code snippet creates a water molecule: hydrogen = Atom ( 'H' , 1 , 1 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) 2b. Make sure that when we print individual molecules, we get something resembling the correct chemical formula (you don't have to take the exact protocol into account). So, e.g. print (water) would render H2O . Make sure that the number 1 is omitted in the representation. 2c In our small implementation, molecules that are created can never change (they are immutable ). However, we can add two molecules together in order to create a new molecule. Implement this method in the class Molecule . Creating molecules this way is, of course, not really possible. However, because of educational reasons, we pretend that this is an ok way to work. You can use the code below to test your implementation: hydrogen = Atom ( 'H' , 1 , 1 ) carbon = Atom ( 'C' , 6 , 6 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) print ( water ) # H2O print ( co2 ) # CO2 print ( water + co2 ) # H2OCO2 Assignment 3: The Chloroplast class \u00b6 As a final assignment, we are going to make a (very, very) simplified version of the photosynthesis process; basically, we are only going to implement the formula stated above. 3a. Create the class Chloroplast . When creating objects of this type, make sure two fields water and co2 are initialised at value 0 . 3b. Implement the following functionality: make a method add_molecule in which we can add water or carbon dioxide molecules. When we add either of them, the corresponding field is incremented by one. When we add something else than water or carbon dioxide, a ValueError is raised, but the program continues to function. If nothing else happens, this method returns an empty list 3c. When we have added a total of 6 CO2-molecules and 12 H2O-molecules, we start of the photosyntheses. We decrease the fields water and co2 with 6 and 12 respectively and create two new molecules: C6H12O6 and O2 (and energy, we we ignore that in this exercise). In this case, the method returns a list of tuples: 1 molecule of sugar and 6 molecules of oxygen (as per the general formula stated above). 3d. Make sure that when we print this instance of chloroplast, we get an idea of how many molecules of water and CO2 are already stored in it. You can use the following script to check your implementation water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) demo = Chloroplast () els = [ water , co2 ] while ( True ): print ( ' \\n What molecule would you like to add?' ) print ( '[1] Water' ) print ( '[2] carbondioxyde' ) print ( 'Please enter your choice: ' , end = '' ) try : choice = int ( input ()) res = demo . add_molecule ( els [ choice - 1 ]) if ( len ( res ) == 0 ): print ( demo ) else : print ( ' \\n === Photosynthesis!' ) print ( res ) print ( demo ) except Exception : print ( ' \\n === That is not a valid choice.' ) Wrap up \u00b6 Make sure your code follows the SOLID principles. Do you see any refactoring possibilities? Did you use constants where possible? Have you identified all the stuff that stayed the same and separated that from all the stuff that changed...? Write a short README.md in which you state all the improvements that can be made to your code base.","title":"Week 1.2: Classes and instances"},{"location":"week1.2.html#week-12-classes-and-instances","text":"","title":"Week 1.2: Classes and instances"},{"location":"week1.2.html#introduction","text":"Photosynthesis is the process by which plants and certain algae convert light energy into chemical energy that can be stored and used to drive the organism's activities. Though different varieties of photosynthesis exist, the overall equation for the type that occurs in plants is as follows: \\(6CO_2 + 6H_2O -> C_6H_{12}O_6 + 6O_2 + energy\\) In this exercise, we are going to create a very simple model of this process.","title":"Introduction"},{"location":"week1.2.html#assignment-1-the-atom-class","text":"1a. Create a class Atom that is a representation of any atom in the periodic table. Make sure that when a concrete atom is instantiated, it is given its symbol, its atomic number and the number of neutrons in the core. Store those parameters in the created object. 1b. Create a method proton_number that returns the number of protons in the nucleus; make another method mass_number that returns the sum of protons and neutrons in the nucleus. Isotopes are types of atoms that have the same number of atomic number but a different number of neutrons in the core. So, e.g. 'normal' hydrogen has 1 proton and 1 neutron, but it also comes in the form of deuterium (1 proton and 2 neutrons) or even tritium (1 proton and 3 neutrons). 1c. Create a method isotope in the class Atom . When this method is called, the normal number of neutrons must be replaced by whatever number is provided to this method. 1d. We define an atom A to be less than another atom B if their proton number is the same (i.e. it is the same element) but the mass number of A is less than the mass number of B. Implement the methods that checks whether two isotopes of the same element are equal to each other, or less than or greater than each other. Raise an exception when the check is called with different types of elements. You can use the code below to test your implementation. protium = Atom ( 'H' , 1 , 1 ) deuterium = Atom ( 'H' , 1 , 2 ) oxygen = Atom ( 'O' , 8 , 8 ) tritium = Atom ( 'H' , 1 , 2 ) tritium . isotope ( 3 ) assert tritium . neutrons == 3 assert tritium . mass_number () == 4 assert protium < deuterium assert deuterium <= tritium assert tritium >= protium print ( oxygen > tritium ) # <-- this should raise an Exception","title":"Assignment 1: the Atom class"},{"location":"week1.2.html#assignment-2-the-molecule-class","text":"A molecule is a neutral group of two or more atoms. 2a. Create the class Molecule . When creating an instance of this class, a list of tuples of two values (a pair ) is given. The first element of this pair is the Atom-object, and the second element is the number of atoms of that type that is put into the molecule. Thus, the following code snippet creates a water molecule: hydrogen = Atom ( 'H' , 1 , 1 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) 2b. Make sure that when we print individual molecules, we get something resembling the correct chemical formula (you don't have to take the exact protocol into account). So, e.g. print (water) would render H2O . Make sure that the number 1 is omitted in the representation. 2c In our small implementation, molecules that are created can never change (they are immutable ). However, we can add two molecules together in order to create a new molecule. Implement this method in the class Molecule . Creating molecules this way is, of course, not really possible. However, because of educational reasons, we pretend that this is an ok way to work. You can use the code below to test your implementation: hydrogen = Atom ( 'H' , 1 , 1 ) carbon = Atom ( 'C' , 6 , 6 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) print ( water ) # H2O print ( co2 ) # CO2 print ( water + co2 ) # H2OCO2","title":"Assignment 2: the Molecule class"},{"location":"week1.2.html#assignment-3-the-chloroplast-class","text":"As a final assignment, we are going to make a (very, very) simplified version of the photosynthesis process; basically, we are only going to implement the formula stated above. 3a. Create the class Chloroplast . When creating objects of this type, make sure two fields water and co2 are initialised at value 0 . 3b. Implement the following functionality: make a method add_molecule in which we can add water or carbon dioxide molecules. When we add either of them, the corresponding field is incremented by one. When we add something else than water or carbon dioxide, a ValueError is raised, but the program continues to function. If nothing else happens, this method returns an empty list 3c. When we have added a total of 6 CO2-molecules and 12 H2O-molecules, we start of the photosyntheses. We decrease the fields water and co2 with 6 and 12 respectively and create two new molecules: C6H12O6 and O2 (and energy, we we ignore that in this exercise). In this case, the method returns a list of tuples: 1 molecule of sugar and 6 molecules of oxygen (as per the general formula stated above). 3d. Make sure that when we print this instance of chloroplast, we get an idea of how many molecules of water and CO2 are already stored in it. You can use the following script to check your implementation water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) demo = Chloroplast () els = [ water , co2 ] while ( True ): print ( ' \\n What molecule would you like to add?' ) print ( '[1] Water' ) print ( '[2] carbondioxyde' ) print ( 'Please enter your choice: ' , end = '' ) try : choice = int ( input ()) res = demo . add_molecule ( els [ choice - 1 ]) if ( len ( res ) == 0 ): print ( demo ) else : print ( ' \\n === Photosynthesis!' ) print ( res ) print ( demo ) except Exception : print ( ' \\n === That is not a valid choice.' )","title":"Assignment 3: The Chloroplast class"},{"location":"week1.2.html#wrap-up","text":"Make sure your code follows the SOLID principles. Do you see any refactoring possibilities? Did you use constants where possible? Have you identified all the stuff that stayed the same and separated that from all the stuff that changed...? Write a short README.md in which you state all the improvements that can be made to your code base.","title":"Wrap up"},{"location":"week1.3.html","text":"Week 1.3: Multiple Class Interaction \u00b6 In this exercise, we will be building a relatively complex system that consists of several classes that need to coordinate certain events. In the process, we introduce the concept of asynchronicity, generators, parallelism and message queues. The central idea is that we have some data that needs to be harvested, transformed and distributed. We build this application in several more or less independent steps, in order to bring the idea home that separation and compartmentalization are good things. Step 1: Producing data \u00b6 Download the file dSST.csv and study its contents. This is a comma-separated data file consisting of the average monthly global temperature anomalies between the years 1880-2021. These temperatures are based on the GISS Surface Temperature Analysis (GISTEMP v4), an estimate of global surface temperature change. Anomalies are defined relative to a base period of 1951-1980. 1a: from CSV to JSON \u00b6 We want this data to be made available in JSON instead of CSV. The big difference between these two data formats is that the first line of a CSV-file contains all the properties ( keys ) of the data, while JSON needs to have these properties repeated. Have a look at the following example: key1,key2 1880,3.14 1881,2.87 which in JSON would be [{ \"key1\" : 1880 , \"key2\" : 3.14 },{ \"key1\" : 1881 , \"key2\" : 2.87 }] Create a class CsvConverter which does exactly this. Make sure that when you create an instance of this class, the header (first line) of a CSV-file is added. Since this is usually just a string, make sure to split this line using str.split(',') in order to create a list of keys. Add a method csv_to_json() which receives a list of lines from a CSV-file and returns this data as JSON. Again, since the lines are just strings, use split to transform each line into a list of values. Make sure that the amount of items in each line corresponds to the number of elements in the header that you stored in the object (make use of assert and issue a nice warning when the numbers don't match). Use the following code as an example for your implementation: >>> keys = 'a,b,c,d' >>> vals = '3,2,1,5' >>> dict ( zip ( keys . split ( ',' ), vals . split ( ',' ))) { 'a' : '3' , 'b' : '2' , 'c' : '1' , 'd' : '5' } >>> You may need to use json.dumps() in your method to make sure that the returned value is indeed valid JSON (past it into jsonparser.org when in doubt). 1b. getting the data \u00b6 Create a class Reader which gets the location of a CSV-file in its constructor (you might want to default this to our dSSST.csv ). Also in the constructor, create an instance of the CvsConcerter and store that in the object's memory space. Reader -objects will read the given CSV-files in strides of a certain amount of lines. In order to do this, you will need to do some bookkeeping within the object itself: you will need to store both the number of lines that are read in one go and also a pointer to the current line in the file. Add a method get_lines() to the Reader -class. In this method, the next stride of lines is returned as JSON. You can use linecache to read certain lines of the data file. So, given that the first line of our CSV-file contains its keys, a flow could be like this (output omitted for readability): red = Reader ( 'dSST.csv' ) red . get_lines () #returns lines 2-6 as json reg . get_lines () #returns lines 7-11 as json reg . get_lines () #returns lines 12-16 as json If there are no more lines in the data file, the method get_lines() needs to return an empty string. Step 2: Consuming the data \u00b6 We are going to create two classes, AverageYear and AverageMonth that make use of this temperature data that is being provided by objects of the Reader -class. There are, of course, several ways in which you can make these Reader -objects available in the consuming classes: we can create a new Reader -object when initializing our consumer, or we can make one Reader -object and give that object to the initializer of the consumers. What do you think are the pros and cons of both approaches? The first consuming class will show the average temperature anomaly for the complete set of five years that our Reader is returning. the second class needs to show the average monthly anomaly for the same five years. Please note that the number of columns is larger than the number of months within the year (the last few columns are some kind of special meteorological averages), so be sure to take the right data. Of course, in both classes you can make use of pandas, numpy, matplotlib and all kind of libraries you are used to in order to make some sort of visualisation. Within both classes, use consecutive calls to get_lines() in order to get the next stride of lines from this data-class. Stop calling this method whenever this method returns an empty string. You don't need to make an incremental plot with each new call to get_lines() (thought that would be a nice addition): it is sufficient when you repaint your visualisation every time new data gets returned. In the end, the architecture would look as follows: There are several issues with this architecture, regardless of the way in which you provided the consuming classes with the Reader -objects. In the following step, we are going to decouple these classes in order to create a more robust and versatile system. Step 3: The observer pattern \u00b6 During the theoretical part, the observer pattern was briefly described. For easy reference, the sequence diagram that was shown is repeated here: In this step, we are going to refactor the classes you've created in step 3 in order to realise this design pattern. 3a: Extending the reader \u00b6 Create two methods in the Reader -class, add_observer() and remove_observer() so that we can add and remove observers. Make sure that when we create a new instance of the Reader , you create an empty set within the memory-space of the Reader . A call to the first method should add the given observer to this list, a call to the second call will (drumm-roll) remove the observer from the list if it is present. If the observer is not present in the list, make sure that nothing happens (removing a non-existent observer from this list is not a reason the crash a system) Next, add a method to notify_observers() to Reader . When called, this method needs to iterate over all the registered observers and call update() on each of them. Now, change the Reader so that the next five lines (the stride ) are produced every five seconds (you can make use of time.sleep(5) in order to accomplish this) and every time new data is read a call to notify_observers() is made. 3b: Extending the consumers \u00b6 Now we need to change the two consumers we created in step 2. Instead of making them responsible for the call to next_lines() , we just let them react to the updates of the Reader -class. Whenever this class has new data available, it will run notify_observers() which in turn will call update() on the observer. Make sure that whenever the update -method is called from the producer, the same visualisation that you made in step 2 is rendered. Please refer to the listing below to get an idea of the infrastructure that is required: prod = Reader ( 'dSST.csv' ) cons1 = AverageYear () cons2 = AverageMonth () prod . register_observer ( cons1 ) prod . register_observer ( cons2 ) If all goes well, some of the problems we encountered earlier are more or less solved. However, this architecture has some problems of its own. Can you see where this goes wrong and how you can solve them?","title":"Week 1.3: Multiple Class Interaction"},{"location":"week1.3.html#week-13-multiple-class-interaction","text":"In this exercise, we will be building a relatively complex system that consists of several classes that need to coordinate certain events. In the process, we introduce the concept of asynchronicity, generators, parallelism and message queues. The central idea is that we have some data that needs to be harvested, transformed and distributed. We build this application in several more or less independent steps, in order to bring the idea home that separation and compartmentalization are good things.","title":"Week 1.3: Multiple Class Interaction"},{"location":"week1.3.html#step-1-producing-data","text":"Download the file dSST.csv and study its contents. This is a comma-separated data file consisting of the average monthly global temperature anomalies between the years 1880-2021. These temperatures are based on the GISS Surface Temperature Analysis (GISTEMP v4), an estimate of global surface temperature change. Anomalies are defined relative to a base period of 1951-1980.","title":"Step 1: Producing data"},{"location":"week1.3.html#1a-from-csv-to-json","text":"We want this data to be made available in JSON instead of CSV. The big difference between these two data formats is that the first line of a CSV-file contains all the properties ( keys ) of the data, while JSON needs to have these properties repeated. Have a look at the following example: key1,key2 1880,3.14 1881,2.87 which in JSON would be [{ \"key1\" : 1880 , \"key2\" : 3.14 },{ \"key1\" : 1881 , \"key2\" : 2.87 }] Create a class CsvConverter which does exactly this. Make sure that when you create an instance of this class, the header (first line) of a CSV-file is added. Since this is usually just a string, make sure to split this line using str.split(',') in order to create a list of keys. Add a method csv_to_json() which receives a list of lines from a CSV-file and returns this data as JSON. Again, since the lines are just strings, use split to transform each line into a list of values. Make sure that the amount of items in each line corresponds to the number of elements in the header that you stored in the object (make use of assert and issue a nice warning when the numbers don't match). Use the following code as an example for your implementation: >>> keys = 'a,b,c,d' >>> vals = '3,2,1,5' >>> dict ( zip ( keys . split ( ',' ), vals . split ( ',' ))) { 'a' : '3' , 'b' : '2' , 'c' : '1' , 'd' : '5' } >>> You may need to use json.dumps() in your method to make sure that the returned value is indeed valid JSON (past it into jsonparser.org when in doubt).","title":"1a: from CSV to JSON"},{"location":"week1.3.html#1b-getting-the-data","text":"Create a class Reader which gets the location of a CSV-file in its constructor (you might want to default this to our dSSST.csv ). Also in the constructor, create an instance of the CvsConcerter and store that in the object's memory space. Reader -objects will read the given CSV-files in strides of a certain amount of lines. In order to do this, you will need to do some bookkeeping within the object itself: you will need to store both the number of lines that are read in one go and also a pointer to the current line in the file. Add a method get_lines() to the Reader -class. In this method, the next stride of lines is returned as JSON. You can use linecache to read certain lines of the data file. So, given that the first line of our CSV-file contains its keys, a flow could be like this (output omitted for readability): red = Reader ( 'dSST.csv' ) red . get_lines () #returns lines 2-6 as json reg . get_lines () #returns lines 7-11 as json reg . get_lines () #returns lines 12-16 as json If there are no more lines in the data file, the method get_lines() needs to return an empty string.","title":"1b. getting the data"},{"location":"week1.3.html#step-2-consuming-the-data","text":"We are going to create two classes, AverageYear and AverageMonth that make use of this temperature data that is being provided by objects of the Reader -class. There are, of course, several ways in which you can make these Reader -objects available in the consuming classes: we can create a new Reader -object when initializing our consumer, or we can make one Reader -object and give that object to the initializer of the consumers. What do you think are the pros and cons of both approaches? The first consuming class will show the average temperature anomaly for the complete set of five years that our Reader is returning. the second class needs to show the average monthly anomaly for the same five years. Please note that the number of columns is larger than the number of months within the year (the last few columns are some kind of special meteorological averages), so be sure to take the right data. Of course, in both classes you can make use of pandas, numpy, matplotlib and all kind of libraries you are used to in order to make some sort of visualisation. Within both classes, use consecutive calls to get_lines() in order to get the next stride of lines from this data-class. Stop calling this method whenever this method returns an empty string. You don't need to make an incremental plot with each new call to get_lines() (thought that would be a nice addition): it is sufficient when you repaint your visualisation every time new data gets returned. In the end, the architecture would look as follows: There are several issues with this architecture, regardless of the way in which you provided the consuming classes with the Reader -objects. In the following step, we are going to decouple these classes in order to create a more robust and versatile system.","title":"Step 2: Consuming the data"},{"location":"week1.3.html#step-3-the-observer-pattern","text":"During the theoretical part, the observer pattern was briefly described. For easy reference, the sequence diagram that was shown is repeated here: In this step, we are going to refactor the classes you've created in step 3 in order to realise this design pattern.","title":"Step 3: The observer pattern"},{"location":"week1.3.html#3a-extending-the-reader","text":"Create two methods in the Reader -class, add_observer() and remove_observer() so that we can add and remove observers. Make sure that when we create a new instance of the Reader , you create an empty set within the memory-space of the Reader . A call to the first method should add the given observer to this list, a call to the second call will (drumm-roll) remove the observer from the list if it is present. If the observer is not present in the list, make sure that nothing happens (removing a non-existent observer from this list is not a reason the crash a system) Next, add a method to notify_observers() to Reader . When called, this method needs to iterate over all the registered observers and call update() on each of them. Now, change the Reader so that the next five lines (the stride ) are produced every five seconds (you can make use of time.sleep(5) in order to accomplish this) and every time new data is read a call to notify_observers() is made.","title":"3a: Extending the reader"},{"location":"week1.3.html#3b-extending-the-consumers","text":"Now we need to change the two consumers we created in step 2. Instead of making them responsible for the call to next_lines() , we just let them react to the updates of the Reader -class. Whenever this class has new data available, it will run notify_observers() which in turn will call update() on the observer. Make sure that whenever the update -method is called from the producer, the same visualisation that you made in step 2 is rendered. Please refer to the listing below to get an idea of the infrastructure that is required: prod = Reader ( 'dSST.csv' ) cons1 = AverageYear () cons2 = AverageMonth () prod . register_observer ( cons1 ) prod . register_observer ( cons2 ) If all goes well, some of the problems we encountered earlier are more or less solved. However, this architecture has some problems of its own. Can you see where this goes wrong and how you can solve them?","title":"3b: Extending the consumers"},{"location":"week1.4.html","text":"Week 1.4: Generators and Map-Reduce \u00b6 Introduction \u00b6 This week, we are going to work with generators and list comprehensions . During the theoretical session, we have seen how we can replace the creation of lists in a for-loop with list-comprehensions. That makes our code more readable, more maintainable, and more pythonic. We have also seen how we can make of generators and iterables to hide the exact implementation of our collection while we are iterating over it. Exercise 1: refactoring your own code. \u00b6 As a warming up exercise, you are asked to replace the for-loops in your elaboration of the exercise of last week with list comprehensions, if you have not already done that. Especially in the consuming classes, it would make sense to use list comprehensions for either the vertical or the horizontal data aggregation. Make sure you know when to use list comprehension and when to use a standard for-loop: both have their place and function in your program, and you should not use comprehensions for the sake of it. Before doing the refactoring, make sure you have pushed your current code to github. While refactoring, make sure your code keeps on working (make use of assert statements where possible). Exercise 2: functions with data \u00b6 Create a function that has two parameters: one for data (you can assume this to be a list) and one for the function to be applied to that data. Let the function return a list with new values that are created by applying the function to all the values. For example, when I give a list [1,2,3,4,5] as a first parameter to this function, and a function to multiply all values of a list by two as a second parameter, the return value of this function should be [2,4,6,8,10] . Now enhance your function so that it can take in an arbitrary number of functions that all need to be applied to the given data. In this second version, the function returns a list of lists: for every application of a given function, there's a list in this return value. So if I were to give two functions to this function, it should return a list of two lists. Make use of list comprehensions in your elaboration. Exercise 3: refactoring other people's code. \u00b6 Download the code for this exercise . This is a script that crawls a website with sport clubs in the city Groningen . It generates a csv-file with the url, phone-number and email-address of 414 sport clubs. Run the script for a few moments to see what it does (make sure to exit it using CTRL-C , otherwise it will take too long). baba@host% python webcrawler.py fetch urls getting sub-urls extracting the data 414 sub-urls https://sport050.nl/sportaanbieders/3x3-unites/ ; 06 -23929416 ; a.einolhagh@live.nl https://sport050.nl/sportaanbieders/40up-fitness/ ; 06 -81484254 ; info@40upfitness.nl https://sport050.nl/sportaanbieders/5ersport/ ; ; 5ersporten@gmail.com?subject = Bericht%20via%20Sport050.nl%20 https://sport050.nl/sportaanbieders/s-exstudiantes/ ; ; https://sport050.nl/sportaanbieders/agsr-gyas/ ; + 050 526 7445 ; Step 1: Simple refactor improvements \u00b6 At the moment, all of the code is working within the global namespace. This is bad for maintainability and re-useability. Create a class Crawler in which you copy the code as it now is. Make a method crawl_site() within this class in which you put the main loop (the one that starts at line 102). Remember to add self as a first parameter to most method of this new class (at least one of the methods can be changed into a @staticmethod ...). Now create a second file main.py in which you import the Crawler -class. Make a new instance of this class and call the main loop method ( crawl_site ) that you have created. If all goes well, the result should be the same as before. Again, make sure to exit the program with CTRL-C after a few runs. At four places at the code, there are lambda expressions; there are even lambda expressions within list comprehension. Remove all of these, so that we only have list comprehensions in the end. Do not replace the main loop with a list comprehension, as we will be changing this method later on. Step 2: Change the loop into an iterator \u00b6 Until now, we had to stop the execution of the code by hand (by hitting CTRL-C ). This is because the main loop (in crawl_site ) runs for all the sub-sites it has found on the site itself. This is, of course, fine if you don't want to do anything else as long as this loop runs, but in order to have more control over the flow of the crawler, it would be better if we were to control the number of sites it needs to parse. Change the method crawl_site() so that it makes use of an internal pointer of the instance (which you need to initialize when you create an instance of the Crawler class). Have a look at the iterator.py -example that you can find via this link to get an idea of how this needs to be done. Note: you need to refactor the loop in crawl_site() in order to make this work. If your implementation is correct, you should be able to run the test-code below without errors: crawler = Crawler () for x in range ( 5 ): print ( str ( next ( crawler ))) #Result: five lines of data When you have completed this refactoring, commit your code to git. Step 3: Make use of a generator \u00b6 During the theoretical session, we have talked about the use of __iter__() and __next__() methods. Implement the __iter__() method in Crawler so that this creates a generator to loop over the crawled websites. Have every call to this iterator return the next crawled website. Have a look at generator.py , that you can find via this link in order to get an idea of how this is to be done. Next, device test code that you can use to have only a few calls to the __iter__() method of the Crawler . You can use zip to accomplish this. Step 4: Come up with enhancements \u00b6 If you look at the code and you think back about the SOLID principles we discussed during our first session, what other refactoring candidates can you spot in this code? Add a small piece of text to your repo in which you analyse this code.","title":"Week 1.4: Generators and Map-Reduce"},{"location":"week1.4.html#week-14-generators-and-map-reduce","text":"","title":"Week 1.4: Generators and Map-Reduce"},{"location":"week1.4.html#introduction","text":"This week, we are going to work with generators and list comprehensions . During the theoretical session, we have seen how we can replace the creation of lists in a for-loop with list-comprehensions. That makes our code more readable, more maintainable, and more pythonic. We have also seen how we can make of generators and iterables to hide the exact implementation of our collection while we are iterating over it.","title":"Introduction"},{"location":"week1.4.html#exercise-1-refactoring-your-own-code","text":"As a warming up exercise, you are asked to replace the for-loops in your elaboration of the exercise of last week with list comprehensions, if you have not already done that. Especially in the consuming classes, it would make sense to use list comprehensions for either the vertical or the horizontal data aggregation. Make sure you know when to use list comprehension and when to use a standard for-loop: both have their place and function in your program, and you should not use comprehensions for the sake of it. Before doing the refactoring, make sure you have pushed your current code to github. While refactoring, make sure your code keeps on working (make use of assert statements where possible).","title":"Exercise 1: refactoring your own code."},{"location":"week1.4.html#exercise-2-functions-with-data","text":"Create a function that has two parameters: one for data (you can assume this to be a list) and one for the function to be applied to that data. Let the function return a list with new values that are created by applying the function to all the values. For example, when I give a list [1,2,3,4,5] as a first parameter to this function, and a function to multiply all values of a list by two as a second parameter, the return value of this function should be [2,4,6,8,10] . Now enhance your function so that it can take in an arbitrary number of functions that all need to be applied to the given data. In this second version, the function returns a list of lists: for every application of a given function, there's a list in this return value. So if I were to give two functions to this function, it should return a list of two lists. Make use of list comprehensions in your elaboration.","title":"Exercise 2: functions with data"},{"location":"week1.4.html#exercise-3-refactoring-other-peoples-code","text":"Download the code for this exercise . This is a script that crawls a website with sport clubs in the city Groningen . It generates a csv-file with the url, phone-number and email-address of 414 sport clubs. Run the script for a few moments to see what it does (make sure to exit it using CTRL-C , otherwise it will take too long). baba@host% python webcrawler.py fetch urls getting sub-urls extracting the data 414 sub-urls https://sport050.nl/sportaanbieders/3x3-unites/ ; 06 -23929416 ; a.einolhagh@live.nl https://sport050.nl/sportaanbieders/40up-fitness/ ; 06 -81484254 ; info@40upfitness.nl https://sport050.nl/sportaanbieders/5ersport/ ; ; 5ersporten@gmail.com?subject = Bericht%20via%20Sport050.nl%20 https://sport050.nl/sportaanbieders/s-exstudiantes/ ; ; https://sport050.nl/sportaanbieders/agsr-gyas/ ; + 050 526 7445 ;","title":"Exercise 3: refactoring other people's code."},{"location":"week1.4.html#step-1-simple-refactor-improvements","text":"At the moment, all of the code is working within the global namespace. This is bad for maintainability and re-useability. Create a class Crawler in which you copy the code as it now is. Make a method crawl_site() within this class in which you put the main loop (the one that starts at line 102). Remember to add self as a first parameter to most method of this new class (at least one of the methods can be changed into a @staticmethod ...). Now create a second file main.py in which you import the Crawler -class. Make a new instance of this class and call the main loop method ( crawl_site ) that you have created. If all goes well, the result should be the same as before. Again, make sure to exit the program with CTRL-C after a few runs. At four places at the code, there are lambda expressions; there are even lambda expressions within list comprehension. Remove all of these, so that we only have list comprehensions in the end. Do not replace the main loop with a list comprehension, as we will be changing this method later on.","title":"Step 1: Simple refactor improvements"},{"location":"week1.4.html#step-2-change-the-loop-into-an-iterator","text":"Until now, we had to stop the execution of the code by hand (by hitting CTRL-C ). This is because the main loop (in crawl_site ) runs for all the sub-sites it has found on the site itself. This is, of course, fine if you don't want to do anything else as long as this loop runs, but in order to have more control over the flow of the crawler, it would be better if we were to control the number of sites it needs to parse. Change the method crawl_site() so that it makes use of an internal pointer of the instance (which you need to initialize when you create an instance of the Crawler class). Have a look at the iterator.py -example that you can find via this link to get an idea of how this needs to be done. Note: you need to refactor the loop in crawl_site() in order to make this work. If your implementation is correct, you should be able to run the test-code below without errors: crawler = Crawler () for x in range ( 5 ): print ( str ( next ( crawler ))) #Result: five lines of data When you have completed this refactoring, commit your code to git.","title":"Step 2: Change the loop into an iterator"},{"location":"week1.4.html#step-3-make-use-of-a-generator","text":"During the theoretical session, we have talked about the use of __iter__() and __next__() methods. Implement the __iter__() method in Crawler so that this creates a generator to loop over the crawled websites. Have every call to this iterator return the next crawled website. Have a look at generator.py , that you can find via this link in order to get an idea of how this is to be done. Next, device test code that you can use to have only a few calls to the __iter__() method of the Crawler . You can use zip to accomplish this.","title":"Step 3: Make use of a generator"},{"location":"week1.4.html#step-4-come-up-with-enhancements","text":"If you look at the code and you think back about the SOLID principles we discussed during our first session, what other refactoring candidates can you spot in this code? Add a small piece of text to your repo in which you analyse this code.","title":"Step 4: Come up with enhancements"},{"location":"week1.5.html","text":"Week 1.5: Divide and Conquer Algorithms \u00b6 Introduction \u00b6","title":"Week 1.5: Divide and Conquer Algorithms"},{"location":"week1.5.html#week-15-divide-and-conquer-algorithms","text":"","title":"Week 1.5: Divide and Conquer Algorithms"},{"location":"week1.5.html#introduction","text":"","title":"Introduction"},{"location":"week2.1.html","text":"Week 2.1: Network operations \u00b6 Introduction \u00b6 In this week, we are going to serve data over the network. It is very improbable that you will do all of your calculations on your own machine, as it can take up a lot of time and resources. That is why we outsource the computation to other machines, so that we only have to wait for the results to come in. As has been explained, in this scenario we need two types of techniques: client-server architecture and asynchronous programming. During the planairy part both of these have been explained and demonstrated; now it is time to integrate the whole shebang and come up with a complete system. Exercise 1: create a server \u00b6 As has been explained during the plenairy part, you can easily create a (rather rudimentary but good enough for our purposes) server using Python's http server module . For easy reference, the most basic setup is repeated below: PORT = 8080 handler = http . server . SimpleHTTPRequestHandler http = socketserver . TCPServer (( \"\" , PORT ), ServerHandler ) print ( \"serving at port\" , PORT ) http . serve_forever () The idea of this exercise it that we are going to make a server that serves the same wheather-data that we used in exercise 1.3 . You can find the datafile using this link . The server will have only one endpoint /data , that you can call with either /all , /{year} or /{from-year}/{to-year} : method endpoint status-code description GET /data/all 200 Ok Gets all the data in de file GET /data/{year} 200 Ok Gets the wheater-data of that particula date \" \" 404 Not found if there is no data for that date \" \" 400 Illegal request if the year is not a year GET /data/{from-year}/{to-year} 200 Ok Gives the wheather-date from from-year to to-year (inclusive) \" \" 404 Not found if there is no data for this range \" \" 400 Illegal request if either from-year or to-year is not a year If a request is done to another endpoint, the server will just respond with a 404. step 1: check the request \u00b6 In the code above, we are using Python's SimpleHTTPRequestHandler to handle our requests. Make a new class that extends SimpleHTTPRequestHandler so that we can have control over our requests. In this new class, create the method go_GET that gets called whenever a GET-request is done to our URI (this is done automatically for you by http.server : please refer to the documentation ). In this method, check the path of the request (using self.path ) to see whether the request in indeed to /data . If this is not the case, respond with a 404 Not found , using the method send_error that is provided in SimpleHTTPRequestHandler . Next, check whether the request ends with /all or contains one or two dates. If this is not the case, we respond with a 400 Bad Request error. step 2: create the data-provider \u00b6 In order to separate the http request-handling from the data-handling, you need to create a new class DataProvider , which is responsible for (you guessed it) providing the data. Give this class a method that contains a parameter on basis of which all or a piece of the wheather data is returned. So this method should give all the data back when it is called with all as a parameter, one line of data when this method is called with only one year for this parameter, and a list of data when it is called with a tuple of two years. Have a look at the code below to get an idea of the workings of this method. If the parameter does not comply to the requirements stated above, it should raise a ValueError . You can use pandas for this class. For now, the method should return a json string. Transpose the json When using to_json in pandas, the returned json is somewhat strange: it uses the index of the DataFrame as a key for every value. In order to make the json more logical, you can transpose the DataFrame before you call to_json . d = DataProvider () print ( d . get_data ()) # returns all the data in de csv as one json-stream print ( d . get_data ( 1991 )) # returns one line of data; the one that corresponds to the year 1991 (line 113) print ( d . get_data ([ 1991 , 2000 ])) # returns 10 lines of data, from the year 1991 to 2000 step 3: use the data-provider in the request handler \u00b6 Now, use this data-provider in the request handler so that its method is called in the correct manner. This means that you need to check the format of the request in your go_GET method and call the data-provider method in the appropriate corresponding manner. If the data-provider raises a ValueError , you need to respond with status-code 400 again. Make sure your server responds with a Status-Code of 200 and a Content-Type header with value application/json . You can use your browser to check the results; just make sure you have the inspector tools open in order to check the headers that your server returns. Alternatively, you can use curl -i to achieve the same result. Exercise 2: create an async client \u00b6 Now, we are going to create a client that calls the end points of the server we have defined above and consumes the results. Make at least two different functions that can operate on the data \u2013 you can use the same methods you have created in exercise 1.3 , or create another interesting metrics for this wheather data. These functions need to return something (like a dict with average temperatures, for example). Create a class NetworkClient that receives the base-url of the server on its initialization (most likely this will be something like http://localhost:8000/data/ ). Provide this class with a method that receives an endpoint from which to fetch the data, and a function that needs to be called when the data has been received. Make sure that this function is non-blocking (in other words, make it an async def and provide awaits where necessary). Have this method return whatever the provided function returns and use that return value in some kind of visualisation (just a dump on the command line will suffice). Finally, create list of several calls to this method and use asyncio.gather to run these calls. Have a look at the example code below to get an idea of the workings: async def main (): tasks = [ demo_delay ( 1 ), get_data ( 'http://jsonplaceholder.typicode.com/todos/1' , print_data ), demo_delay ( 2 ), get_data ( 'http://jsonplaceholder.typicode.com/todos/2' , print_data ), demo_delay ( 3 ), ] await asyncio . gather ( * tasks ) if __name__ == '__main__' : asyncio . run ( main ())","title":"Week 2.1: Network operations"},{"location":"week2.1.html#week-21-network-operations","text":"","title":"Week 2.1: Network operations"},{"location":"week2.1.html#introduction","text":"In this week, we are going to serve data over the network. It is very improbable that you will do all of your calculations on your own machine, as it can take up a lot of time and resources. That is why we outsource the computation to other machines, so that we only have to wait for the results to come in. As has been explained, in this scenario we need two types of techniques: client-server architecture and asynchronous programming. During the planairy part both of these have been explained and demonstrated; now it is time to integrate the whole shebang and come up with a complete system.","title":"Introduction"},{"location":"week2.1.html#exercise-1-create-a-server","text":"As has been explained during the plenairy part, you can easily create a (rather rudimentary but good enough for our purposes) server using Python's http server module . For easy reference, the most basic setup is repeated below: PORT = 8080 handler = http . server . SimpleHTTPRequestHandler http = socketserver . TCPServer (( \"\" , PORT ), ServerHandler ) print ( \"serving at port\" , PORT ) http . serve_forever () The idea of this exercise it that we are going to make a server that serves the same wheather-data that we used in exercise 1.3 . You can find the datafile using this link . The server will have only one endpoint /data , that you can call with either /all , /{year} or /{from-year}/{to-year} : method endpoint status-code description GET /data/all 200 Ok Gets all the data in de file GET /data/{year} 200 Ok Gets the wheater-data of that particula date \" \" 404 Not found if there is no data for that date \" \" 400 Illegal request if the year is not a year GET /data/{from-year}/{to-year} 200 Ok Gives the wheather-date from from-year to to-year (inclusive) \" \" 404 Not found if there is no data for this range \" \" 400 Illegal request if either from-year or to-year is not a year If a request is done to another endpoint, the server will just respond with a 404.","title":"Exercise 1: create a server"},{"location":"week2.1.html#step-1-check-the-request","text":"In the code above, we are using Python's SimpleHTTPRequestHandler to handle our requests. Make a new class that extends SimpleHTTPRequestHandler so that we can have control over our requests. In this new class, create the method go_GET that gets called whenever a GET-request is done to our URI (this is done automatically for you by http.server : please refer to the documentation ). In this method, check the path of the request (using self.path ) to see whether the request in indeed to /data . If this is not the case, respond with a 404 Not found , using the method send_error that is provided in SimpleHTTPRequestHandler . Next, check whether the request ends with /all or contains one or two dates. If this is not the case, we respond with a 400 Bad Request error.","title":"step 1: check the request"},{"location":"week2.1.html#step-2-create-the-data-provider","text":"In order to separate the http request-handling from the data-handling, you need to create a new class DataProvider , which is responsible for (you guessed it) providing the data. Give this class a method that contains a parameter on basis of which all or a piece of the wheather data is returned. So this method should give all the data back when it is called with all as a parameter, one line of data when this method is called with only one year for this parameter, and a list of data when it is called with a tuple of two years. Have a look at the code below to get an idea of the workings of this method. If the parameter does not comply to the requirements stated above, it should raise a ValueError . You can use pandas for this class. For now, the method should return a json string. Transpose the json When using to_json in pandas, the returned json is somewhat strange: it uses the index of the DataFrame as a key for every value. In order to make the json more logical, you can transpose the DataFrame before you call to_json . d = DataProvider () print ( d . get_data ()) # returns all the data in de csv as one json-stream print ( d . get_data ( 1991 )) # returns one line of data; the one that corresponds to the year 1991 (line 113) print ( d . get_data ([ 1991 , 2000 ])) # returns 10 lines of data, from the year 1991 to 2000","title":"step 2: create the data-provider"},{"location":"week2.1.html#step-3-use-the-data-provider-in-the-request-handler","text":"Now, use this data-provider in the request handler so that its method is called in the correct manner. This means that you need to check the format of the request in your go_GET method and call the data-provider method in the appropriate corresponding manner. If the data-provider raises a ValueError , you need to respond with status-code 400 again. Make sure your server responds with a Status-Code of 200 and a Content-Type header with value application/json . You can use your browser to check the results; just make sure you have the inspector tools open in order to check the headers that your server returns. Alternatively, you can use curl -i to achieve the same result.","title":"step 3: use the data-provider in the request handler"},{"location":"week2.1.html#exercise-2-create-an-async-client","text":"Now, we are going to create a client that calls the end points of the server we have defined above and consumes the results. Make at least two different functions that can operate on the data \u2013 you can use the same methods you have created in exercise 1.3 , or create another interesting metrics for this wheather data. These functions need to return something (like a dict with average temperatures, for example). Create a class NetworkClient that receives the base-url of the server on its initialization (most likely this will be something like http://localhost:8000/data/ ). Provide this class with a method that receives an endpoint from which to fetch the data, and a function that needs to be called when the data has been received. Make sure that this function is non-blocking (in other words, make it an async def and provide awaits where necessary). Have this method return whatever the provided function returns and use that return value in some kind of visualisation (just a dump on the command line will suffice). Finally, create list of several calls to this method and use asyncio.gather to run these calls. Have a look at the example code below to get an idea of the workings: async def main (): tasks = [ demo_delay ( 1 ), get_data ( 'http://jsonplaceholder.typicode.com/todos/1' , print_data ), demo_delay ( 2 ), get_data ( 'http://jsonplaceholder.typicode.com/todos/2' , print_data ), demo_delay ( 3 ), ] await asyncio . gather ( * tasks ) if __name__ == '__main__' : asyncio . run ( main ())","title":"Exercise 2: create an async client"},{"location":"week2.2.html","text":"Week 2.2: Parallellisation \u00b6 Introduction \u00b6 In the plenairy part we have discussed how you can make use of Python's multiprocessor module to distribute the computation of your problem over multiple cores on your CPU. We have seen how this increased performances when you are working with a lot of data, or when you need to do complex calculations. Even though the multiprocessor module is quite complex, the basic workings of it is quite simple. You provide your code with the number of cores you wish to use, some data you want to work with and a function that needs to be called for all the elements of your data. You use the function map for this minimal working example. For easy reference, the relevant portion of the code of the plenairy part is repeated below: import multiprocessor as mp with mp . Pool () as p : res = p . map ( sum_squared , numbers ) In this exercise we are going to make use of the biopython package to harvest the first ten references quoted by a random article on the National Center for Biotechnology Information . Because harvesting and saving files can be a time consuming process, it makes sense use multiple cores for this task. step 0: obtaining an API key \u00b6 Alas, since a few years having an API key is required to automatically download articles from the NCBI; such a key enables a data-provider to monitor who is using what data. However, it is good to practice this procedure, since you will no doubt come across this at other times in your career. You don't need to program anything for this step: we will just guide you throught it. Go to the signin-page of the NBCI , scroll down a little bit until you see the button more login options . Click on that button and on the next page search for 'Hanze'; you will see an option to log in with your Hanze account (which is basically a Microsoft account). Log in with your Hanze credentials: Once you are logged in, click on your profile image in the top right corner and in the drop-down that appears select Account settings . Scroll down a little bit until you see a button Create an API Key . When you click this button, your API key is generated and displayed. Copy this string and save it on a convenient place: you will need to provide this key for every request to the NCBI. step 1: install and use the biopython package \u00b6 As said, in this exercise we are going to use the biopython package . You can just pip install this package and start working with it. Biopython is a very useful, but very HUGE library on dealing with biomedical data. Because this assignment really is meant to get you to use the multiprocessing facilities, rather than trying to teach you to use Biopython itself, we will provide you with the necessary code that you can tinker to fullfill the assignment. However, we urge you to study the package in its totality, as it will surely come in handy at times. Specifically, we will be using the Entrez class of the library. This class is an API to the query and database system of the NBCI . It can be used to query all kinds of databases and systems. Have a look at the code below to see how we can query the pubmed database for a specific article (one with an id of '30049270\"' (note: the id's are strings)): from Bio import Entrez # the next two lines are needed to create an environment in which the # ssl doesn't complain about non-existing public keys... import ssl ssl . _create_default_https_context = ssl . _create_unverified_context #enter your email here; the one you used to create an api key in step 0 Entrez . email = '' file = Entrez . elink ( dbfrom = \"pubmed\" , db = \"pmc\" , LinkName = \"pubmed_pmc_refs\" , id = \"30049270\" , api_key = '<YOUR API KEY HERE>' results = Entrez . read ( file ) print ( results ) In this case, we use elink ; this doesn't give us the whole article, but only information that is related to the article. We can, for example, use this to get all the ID s of all the articles that are referenced in this one: references = [ f ' { link [ \"Id\" ] } ' for link in results [ 0 ][ \"LinkSetDb\" ][ 0 ][ \"Link\" ]] print ( references ) If we wanted to obtain the article with authors and abstract, we should have used efetch . In the code below, we receive all the information of the article in question in xml-format: handle = Entrez . efetch ( db = \"pubmed\" , id = '30049270' , retmode = \"xml\" , api_key = '<YOUR API KEY HERE>' ) print ( handle . read ()) step 2: download the referenced articles \u00b6 With the examples above, you should be able to perform the exercise itself. Make use of the Entrez class to obtain the ten first references of an article. You need to be able to provide your script with pubmed id. Your script will look up the references of that article and download the full record of the first ten of these (in xml format). It needs to save these ten full records on your file system, with the id of the reference as its filename (and xml as its extension). So if I were to run your script with pumed-id '30049270' , the listing of the directory would look somewhat like the following (of course, your hostname, username and path will differ from ours): ( masterdt ) baba@grisea Exercise % ls -l total 136 10013231 .xml 10023427 .xml 10029973 .xml 9788541 .xml 9821373 .xml 9843110 .xml 9884887 .xml 9892353 .xml 9970664 .xml 9983048 .xml ( masterdt ) baba@grisea Exercise % You must use the multiprocessor package for the download of these articles. extra challenge \u00b6 If you want and have some time to spare, try to do the same exercise without the multiprocessor package. use time to measure the time it took to execute both versions; do you see a difference?","title":"Week 2.2: Parallellisation"},{"location":"week2.2.html#week-22-parallellisation","text":"","title":"Week 2.2: Parallellisation"},{"location":"week2.2.html#introduction","text":"In the plenairy part we have discussed how you can make use of Python's multiprocessor module to distribute the computation of your problem over multiple cores on your CPU. We have seen how this increased performances when you are working with a lot of data, or when you need to do complex calculations. Even though the multiprocessor module is quite complex, the basic workings of it is quite simple. You provide your code with the number of cores you wish to use, some data you want to work with and a function that needs to be called for all the elements of your data. You use the function map for this minimal working example. For easy reference, the relevant portion of the code of the plenairy part is repeated below: import multiprocessor as mp with mp . Pool () as p : res = p . map ( sum_squared , numbers ) In this exercise we are going to make use of the biopython package to harvest the first ten references quoted by a random article on the National Center for Biotechnology Information . Because harvesting and saving files can be a time consuming process, it makes sense use multiple cores for this task.","title":"Introduction"},{"location":"week2.2.html#step-0-obtaining-an-api-key","text":"Alas, since a few years having an API key is required to automatically download articles from the NCBI; such a key enables a data-provider to monitor who is using what data. However, it is good to practice this procedure, since you will no doubt come across this at other times in your career. You don't need to program anything for this step: we will just guide you throught it. Go to the signin-page of the NBCI , scroll down a little bit until you see the button more login options . Click on that button and on the next page search for 'Hanze'; you will see an option to log in with your Hanze account (which is basically a Microsoft account). Log in with your Hanze credentials: Once you are logged in, click on your profile image in the top right corner and in the drop-down that appears select Account settings . Scroll down a little bit until you see a button Create an API Key . When you click this button, your API key is generated and displayed. Copy this string and save it on a convenient place: you will need to provide this key for every request to the NCBI.","title":"step 0: obtaining an API key"},{"location":"week2.2.html#step-1-install-and-use-the-biopython-package","text":"As said, in this exercise we are going to use the biopython package . You can just pip install this package and start working with it. Biopython is a very useful, but very HUGE library on dealing with biomedical data. Because this assignment really is meant to get you to use the multiprocessing facilities, rather than trying to teach you to use Biopython itself, we will provide you with the necessary code that you can tinker to fullfill the assignment. However, we urge you to study the package in its totality, as it will surely come in handy at times. Specifically, we will be using the Entrez class of the library. This class is an API to the query and database system of the NBCI . It can be used to query all kinds of databases and systems. Have a look at the code below to see how we can query the pubmed database for a specific article (one with an id of '30049270\"' (note: the id's are strings)): from Bio import Entrez # the next two lines are needed to create an environment in which the # ssl doesn't complain about non-existing public keys... import ssl ssl . _create_default_https_context = ssl . _create_unverified_context #enter your email here; the one you used to create an api key in step 0 Entrez . email = '' file = Entrez . elink ( dbfrom = \"pubmed\" , db = \"pmc\" , LinkName = \"pubmed_pmc_refs\" , id = \"30049270\" , api_key = '<YOUR API KEY HERE>' results = Entrez . read ( file ) print ( results ) In this case, we use elink ; this doesn't give us the whole article, but only information that is related to the article. We can, for example, use this to get all the ID s of all the articles that are referenced in this one: references = [ f ' { link [ \"Id\" ] } ' for link in results [ 0 ][ \"LinkSetDb\" ][ 0 ][ \"Link\" ]] print ( references ) If we wanted to obtain the article with authors and abstract, we should have used efetch . In the code below, we receive all the information of the article in question in xml-format: handle = Entrez . efetch ( db = \"pubmed\" , id = '30049270' , retmode = \"xml\" , api_key = '<YOUR API KEY HERE>' ) print ( handle . read ())","title":"step 1: install and use the biopython package"},{"location":"week2.2.html#step-2-download-the-referenced-articles","text":"With the examples above, you should be able to perform the exercise itself. Make use of the Entrez class to obtain the ten first references of an article. You need to be able to provide your script with pubmed id. Your script will look up the references of that article and download the full record of the first ten of these (in xml format). It needs to save these ten full records on your file system, with the id of the reference as its filename (and xml as its extension). So if I were to run your script with pumed-id '30049270' , the listing of the directory would look somewhat like the following (of course, your hostname, username and path will differ from ours): ( masterdt ) baba@grisea Exercise % ls -l total 136 10013231 .xml 10023427 .xml 10029973 .xml 9788541 .xml 9821373 .xml 9843110 .xml 9884887 .xml 9892353 .xml 9970664 .xml 9983048 .xml ( masterdt ) baba@grisea Exercise % You must use the multiprocessor package for the download of these articles.","title":"step 2: download the referenced articles"},{"location":"week2.2.html#extra-challenge","text":"If you want and have some time to spare, try to do the same exercise without the multiprocessor package. use time to measure the time it took to execute both versions; do you see a difference?","title":"extra challenge"},{"location":"week2.3.html","text":"Week 2.3: Dask \u00b6 Introduction \u00b6 This week and the next, we are talking about Dask : a flexible library for parallal computing in Python. The first part of this two-part invention we are working on our own computer; the second part we will look into methods of making use of the network to really distribute our calculations. The exercises of this week are just meant to get some hands-on experience with Dask, so the elaborations do not form part of your graded portfolio. You need to install Dask using conda or pip; have look at the documentation to see how that is done. Also, for the second part of this exercise we are going to work with dask_ml , so you can perhaps install that while you're at it. We also make use of sklearn , so in case you haven't done already you should install this as well. Excercise 1: from pandas to dask DataFrame \u00b6 In this notebook, you will be working with the New York City Airline data. This dataset is only about 200MB, so that you can download it in a reasonable time, but dask.dataframe will scale to datasets much larger than memory. Download and run the prep.py-script , either in a Notebook or from the command line. This little script will download ten csv-files that make up the Airline data. Have a look at one of them to get an idea of how they look. As we have demonstrated during the plenary part, you need to setup a local cluster for Dask to work with. The relevant code is repeated below. Run this code and open a browser window to the link that is will provide. from dask.distributed import Client client = Client ( n_workers = 4 ) client Read all the flights-data that you have just downloaded in a Dask dataframe \u2013 by convention they are called ddf (instead of df for regular Pandas dataframe s). You should use read_csv as you are used to, but this time use it on your Dask dataframe. What happens when you do ddf.head() and can you explain the difference with a regular pandas dataframe ? Also, use ddf.visualise() to get an idea of what the dataframe looks like. As is explained during the plenary part, you need to call compute() in orde to actually do something with the data in your ddf . Some functions, such as len() or head() implicitely trigger a call to compute() (can you imagine why?). However, in this particular case when you call ddf.tail() , the system will respond with an error. Unlike pandas.read_csv which reads in the entire file before inferring datatypes, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions. In this case, the datatypes inferred in the sample are incorrect. The first n rows have no value for CRSElapsedTime (which pandas infers as a float ), and later on turn out to be strings ( object dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options: Specify dtypes directly using the dtype keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant. Increase the size of the sample keyword (in bytes) Use assume_missing to make dask assume that columns inferred to be int (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply. Use the first option and specify the correct datatypes for TailNum , CRSElapsedTime and Cancelled : ddf = dd . read_csv ( os . path . join ( \"data\" , \"nycflights\" , \"*.csv\" ), parse_dates = { \"Date\" : [ 0 , 1 , 2 ]}, dtype = { \"TailNum\" : str , \"CRSElapsedTime\" : float , \"Cancelled\" : bool }, ) Now, use your pandas-knowledge to aswer the questions below. Don't forget to have the client-status-window open so that you can see what is happening. Remember you have to call compute() in order to actually get the results... In total, how many non-canceled flights were taken from each airport? What was the average departure delay from each airport? Per airport, what day of the week has the worst average departure delay? What are the busiest hours? Exercise 2: Dask_ML \u00b6 step 1: use dask as a backend \u00b6 The second part of these exerises are about dask_ml . Again, this is more in order to get some hands-on experience using the library. Begin by creating a new client and opening that in a new window. We start by using GridSearchCV to determine the best parameters for a Support Vector Classifier to classify some random data. Have a look at the code below: X , y = make_classification ( n_samples = 1000 , random_state = 0 ) param_grid = { \"C\" : [ 0.001 , 0.01 , 0.1 , 0.5 , 1.0 , 2.0 , 5.0 , 10.0 ], \"kernel\" : [ 'rbf' , 'poly' , 'sigmoid' ], \"shrinking\" : [ True , False ]} grid_search = GridSearchCV ( SVC ( gamma = 'auto' , random_state = 0 , probability = True ), param_grid = param_grid , return_train_score = False , cv = 3 , n_jobs =- 1 ) grid_search . fit ( X , y ) Scikit-learn uses joblib for single-machine parallelism. This lets you train most estimators (anything that accepts an n_jobs parameter) using all the cores of your laptop or workstation. Alternatively, Scikit-Learn can use Dask for parallelism. This lets you train those estimators using all the cores of your cluster without significantly changing your code. In that case, you have to provide the string dask to the method parallel_backend . Again, have the client-status-window open in order to monitor the workings. import joblib with joblib . parallel_backend ( 'dask' ): #YOUR CODE HERE In either case, you can use the cv_results_ -property of the trained model to investigate the best parameters for the classification (even though that is not really the purpose for this exercise). Use pd.DataFrame(grid_search.cv_results_) to put the results in a dataframe. step 2: classify with dask_ml \u00b6 In this last exercise, we will create a classifier based on a sklearn moon-dataset . Download the file [ moons-csv](files/moons-data.csv) and load this in a pandas dataframe. Create a numpy matrix X on basis of the first two column of this dataframe, and a number array y on basis on the third. The X holds our data and our y` the classes. Use sklearn.model_selection.train_test_split() to split the data in two seperate parts, with 80% trainings-data and 20% test-data. Make two logistic regressors, one from sklearn and one from ml_dask . Train both of these regressors on the trainings-data and check their validity on basis of the test-data. Experiment with different values for the hyperparameters. Do you see a difference between the results? Lastly, plot a scatter-plot of the test-data including the classes (either predicted of actual).","title":"Week 2.3: Dask"},{"location":"week2.3.html#week-23-dask","text":"","title":"Week 2.3: Dask"},{"location":"week2.3.html#introduction","text":"This week and the next, we are talking about Dask : a flexible library for parallal computing in Python. The first part of this two-part invention we are working on our own computer; the second part we will look into methods of making use of the network to really distribute our calculations. The exercises of this week are just meant to get some hands-on experience with Dask, so the elaborations do not form part of your graded portfolio. You need to install Dask using conda or pip; have look at the documentation to see how that is done. Also, for the second part of this exercise we are going to work with dask_ml , so you can perhaps install that while you're at it. We also make use of sklearn , so in case you haven't done already you should install this as well.","title":"Introduction"},{"location":"week2.3.html#excercise-1-from-pandas-to-dask-dataframe","text":"In this notebook, you will be working with the New York City Airline data. This dataset is only about 200MB, so that you can download it in a reasonable time, but dask.dataframe will scale to datasets much larger than memory. Download and run the prep.py-script , either in a Notebook or from the command line. This little script will download ten csv-files that make up the Airline data. Have a look at one of them to get an idea of how they look. As we have demonstrated during the plenary part, you need to setup a local cluster for Dask to work with. The relevant code is repeated below. Run this code and open a browser window to the link that is will provide. from dask.distributed import Client client = Client ( n_workers = 4 ) client Read all the flights-data that you have just downloaded in a Dask dataframe \u2013 by convention they are called ddf (instead of df for regular Pandas dataframe s). You should use read_csv as you are used to, but this time use it on your Dask dataframe. What happens when you do ddf.head() and can you explain the difference with a regular pandas dataframe ? Also, use ddf.visualise() to get an idea of what the dataframe looks like. As is explained during the plenary part, you need to call compute() in orde to actually do something with the data in your ddf . Some functions, such as len() or head() implicitely trigger a call to compute() (can you imagine why?). However, in this particular case when you call ddf.tail() , the system will respond with an error. Unlike pandas.read_csv which reads in the entire file before inferring datatypes, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions. In this case, the datatypes inferred in the sample are incorrect. The first n rows have no value for CRSElapsedTime (which pandas infers as a float ), and later on turn out to be strings ( object dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options: Specify dtypes directly using the dtype keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant. Increase the size of the sample keyword (in bytes) Use assume_missing to make dask assume that columns inferred to be int (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply. Use the first option and specify the correct datatypes for TailNum , CRSElapsedTime and Cancelled : ddf = dd . read_csv ( os . path . join ( \"data\" , \"nycflights\" , \"*.csv\" ), parse_dates = { \"Date\" : [ 0 , 1 , 2 ]}, dtype = { \"TailNum\" : str , \"CRSElapsedTime\" : float , \"Cancelled\" : bool }, ) Now, use your pandas-knowledge to aswer the questions below. Don't forget to have the client-status-window open so that you can see what is happening. Remember you have to call compute() in order to actually get the results... In total, how many non-canceled flights were taken from each airport? What was the average departure delay from each airport? Per airport, what day of the week has the worst average departure delay? What are the busiest hours?","title":"Excercise 1: from pandas to dask DataFrame"},{"location":"week2.3.html#exercise-2-dask_ml","text":"","title":"Exercise 2: Dask_ML"},{"location":"week2.3.html#step-1-use-dask-as-a-backend","text":"The second part of these exerises are about dask_ml . Again, this is more in order to get some hands-on experience using the library. Begin by creating a new client and opening that in a new window. We start by using GridSearchCV to determine the best parameters for a Support Vector Classifier to classify some random data. Have a look at the code below: X , y = make_classification ( n_samples = 1000 , random_state = 0 ) param_grid = { \"C\" : [ 0.001 , 0.01 , 0.1 , 0.5 , 1.0 , 2.0 , 5.0 , 10.0 ], \"kernel\" : [ 'rbf' , 'poly' , 'sigmoid' ], \"shrinking\" : [ True , False ]} grid_search = GridSearchCV ( SVC ( gamma = 'auto' , random_state = 0 , probability = True ), param_grid = param_grid , return_train_score = False , cv = 3 , n_jobs =- 1 ) grid_search . fit ( X , y ) Scikit-learn uses joblib for single-machine parallelism. This lets you train most estimators (anything that accepts an n_jobs parameter) using all the cores of your laptop or workstation. Alternatively, Scikit-Learn can use Dask for parallelism. This lets you train those estimators using all the cores of your cluster without significantly changing your code. In that case, you have to provide the string dask to the method parallel_backend . Again, have the client-status-window open in order to monitor the workings. import joblib with joblib . parallel_backend ( 'dask' ): #YOUR CODE HERE In either case, you can use the cv_results_ -property of the trained model to investigate the best parameters for the classification (even though that is not really the purpose for this exercise). Use pd.DataFrame(grid_search.cv_results_) to put the results in a dataframe.","title":"step 1: use dask as a backend"},{"location":"week2.3.html#step-2-classify-with-dask_ml","text":"In this last exercise, we will create a classifier based on a sklearn moon-dataset . Download the file [ moons-csv](files/moons-data.csv) and load this in a pandas dataframe. Create a numpy matrix X on basis of the first two column of this dataframe, and a number array y on basis on the third. The X holds our data and our y` the classes. Use sklearn.model_selection.train_test_split() to split the data in two seperate parts, with 80% trainings-data and 20% test-data. Make two logistic regressors, one from sklearn and one from ml_dask . Train both of these regressors on the trainings-data and check their validity on basis of the test-data. Experiment with different values for the hyperparameters. Do you see a difference between the results? Lastly, plot a scatter-plot of the test-data including the classes (either predicted of actual).","title":"step 2: classify with dask_ml"},{"location":"week2.4.html","text":"Week 2.4: More on Dask \u00b6 Introduction \u00b6 Last week, we had our introduction on Dask. We saw how we can employ Dask to parallize our computation on large datasets. This week, we are going to look at some optimalization steps you can use to further decrease the time it takes to have your data processed. During the plenary part, we have discussed a few possible steps you can take to optimize your data and reduce the time to compute certain features. In the following exercises, we are going to use six transformations on our data and compute the time it takes for the same operation after each step. This way, we can get a nice feeling of the reduction of the computation time. step 0a: get the data \u00b6 Download the script create_groupby_dask.py and study its contents. You can use this script to generate large randomized datasets that contain a certain amount of groups (so that we can experiment with groupby in either Dask or Pandas). If you run this script in the following manner, a dataset of about fivehunderd megabytes will be create in the directory test (of course, you can change this directory to your liking): python create_groupby_dask.py -n 10000000 -k 100 -nf 1 -dir test The script will create a file with a name that mirrors the setting you used in your command; so test/groupby-N_10000000_K_100_file_0.csv means this is a dataset with 10_000_000 records ( N_10000000 ) and hunderd groups ( K_100 ). Now load the data in a pandas dataframe to get an idea of its contents. In [ 1 ]: import pandas as pd In [ 2 ]: df = pd . read_csv ( 'test/groupby-N_1000000_K_100_file_0.csv' ) In [ 3 ]: df . head () Out [ 3 ]: id1 id2 id3 id4 id5 id6 v1 v2 v3 0 id066 id009 id0000005920 59 62 1719 4 14 41.904603 1 id029 id027 id0000003135 87 44 1553 0 10 77.563258 2 id026 id089 id0000008985 39 0 5118 1 8 67.640243 3 id010 id002 id0000005883 49 19 1774 4 14 58.275440 4 id092 id052 id0000003463 59 83 9388 3 13 86.269280 In [ 4 ]: step 0b: create a timing function \u00b6 As we are creating a benchmark in this exercise, we are going to perform the same operation on the dataframe (either Dask or Pandas) multiple times, in which we are only interested in the time it took to perform that operation. So, it makes sense to create a function that receives a dataframe, performs the operation and returns the time it took to finish. Create a function perform_test() that does exactly this. During this benchmark, we are going to group the dataframe on id1 and sum up the values of v1 . Note: since this function needs to be called with both a pandas dataframe and a dask dataframe, you need to take into account that the last one uses lazy evaluation; so if the dataframe is from dask, you need to call compute() in the end. Use isinstance to check which type of dataframe you are receiving. Exercise 1: improve on the Dask dataframe \u00b6 step 1: setting a dask baseline \u00b6 Let\u2019s run the groupby query with Dask. We\u2019re going to intentionally type all the columns except v1 as object columns, which should give us a good worst case Dask performance benchmark. object columns are notoriously memory hungry and inefficient. Use dask.read_csv with the following dictionary for the ntype -parameter to load our test-dataset and change all the columns into obect s. dtypes = { \"id1\" : \"object\" , \"id2\" : \"object\" , \"id3\" : \"object\" , \"id4\" : \"object\" , \"id5\" : \"object\" , \"id6\" : \"object\" , \"v1\" : \"int64\" , \"v2\" : \"object\" , \"v3\" : \"object\" , } As you no doubt will see, this computation takes far longer than our standard pandas implementation. Now let's see if we can speed this up a little bit. step 2: avoid object columns \u00b6 The datatype object is notoriously inefficient when it comes to computation and calculation. If we know the data-type of a column, it is always better to provide exactly this. We can type id1 , id2 , and id3 as string[pyarrow] type columns, which are way more efficient. The column v3 seems to be a float64 and the rest of the columns can be seen as int64 . Change the parameter dtype to reflect these improvements and rerun the test. Do you see an increase in performance? step 3: using multiple files \u00b6 Dask can read and write multiple files in parallel. Since parallel I/O is normally a lot faster than working with a single large file, it can make sense to split the data over multiple files. Use the ddf.repartition() method with a partition_size of '100MB' to create several different files. Look at the test -directory (or whereever you have stored the files) in order to see the files this method has created. Now, use read_csv with a wildcard to create a dask dataframe with all these newly created files (don't forget to include the correct datatype from the previous exercise). Next, run the perform_test again. step 4a: parquet instead of csv \u00b6 CSV is actually a bad data format. It is, as we say, row based , so it is impossible to drop certain columns while reading in the data. Columnar file formats that are stored as binary usually perform better than row-based, text file formats like CSV. Compressing the files to create smaller file sizes also helps. Read more about dast dataframes and Parquet on this documentation site . Use dask.to_parquet in the same manner as you have created the several csv-files in the previous exercise. For now, use compression=None as a parameter. Also, use pyarrow as an engine (you might need to pip install this). Next, use dask.read_parquet to read in the parquet-files (remember to use the same engine you used when creating the files) and perform the test again. step 4b: use snappy as a compressor \u00b6 Recreate the parquet-files using snappy (which you also probably need to pip install : be sure to install python-snappy ) as an engine and reperform the test. Is there an increase in the efficiency? step 5: column pruning \u00b6 In contrast to csv, parquet is a columnar file format, which means you can selectively grab certain columns from the file. This is commonly referred to as column pruning . Column pruning isn\u2019t possible for row based file formats like CSV. Have a look at the documentation for dask.read_csv to see how you can select only the columns that are relevant for our query: id1 and v1 . Create a new dataframe from our parquet-files with only these columns and, again, perform the test. step 6: comparison with pandas \u00b6 Now, after all this work, we need to see whether dask is actually performing better than pandas. Create a dataframe on basis of the csv-file and run our perform_test -function. Note the time it takes to execute; is this better than, just as good as, or worse than dask...? You will probably see that pandas performs way better than dask, even with the last improvements on the data. So why go through all the hassle? You will see that when you redo the exercise with ten times as much data as the dataset that we had. This means that you have to take not only the computation you are doing into account when thinking about Dask, but also about the size of your dataset. It only makes sense to use Dask when your dataset is really large. Exercise 2: Lazy evaluation \u00b6 During the planery part, we have talked about the possibility of lazy evaluation and parallel computing. Reformat your test code so that you make use of this techniques for the computation \u2013 see the relevant code below. Do you see an increase in the performance of Dask? import dask lazy_results = [] for parameters in input_params . values : lazy_result = dask . delayed ( costly_simulation )( parameters ) lazy_results . append ( lazy_result ) futures = dask . persist ( * lazy_results ) You can even try to increase the number of clusters on your local machine, using client.cluster.scale() , as was shown in the plenary part.","title":"Week 2.4: More on Dask"},{"location":"week2.4.html#week-24-more-on-dask","text":"","title":"Week 2.4: More on Dask"},{"location":"week2.4.html#introduction","text":"Last week, we had our introduction on Dask. We saw how we can employ Dask to parallize our computation on large datasets. This week, we are going to look at some optimalization steps you can use to further decrease the time it takes to have your data processed. During the plenary part, we have discussed a few possible steps you can take to optimize your data and reduce the time to compute certain features. In the following exercises, we are going to use six transformations on our data and compute the time it takes for the same operation after each step. This way, we can get a nice feeling of the reduction of the computation time.","title":"Introduction"},{"location":"week2.4.html#step-0a-get-the-data","text":"Download the script create_groupby_dask.py and study its contents. You can use this script to generate large randomized datasets that contain a certain amount of groups (so that we can experiment with groupby in either Dask or Pandas). If you run this script in the following manner, a dataset of about fivehunderd megabytes will be create in the directory test (of course, you can change this directory to your liking): python create_groupby_dask.py -n 10000000 -k 100 -nf 1 -dir test The script will create a file with a name that mirrors the setting you used in your command; so test/groupby-N_10000000_K_100_file_0.csv means this is a dataset with 10_000_000 records ( N_10000000 ) and hunderd groups ( K_100 ). Now load the data in a pandas dataframe to get an idea of its contents. In [ 1 ]: import pandas as pd In [ 2 ]: df = pd . read_csv ( 'test/groupby-N_1000000_K_100_file_0.csv' ) In [ 3 ]: df . head () Out [ 3 ]: id1 id2 id3 id4 id5 id6 v1 v2 v3 0 id066 id009 id0000005920 59 62 1719 4 14 41.904603 1 id029 id027 id0000003135 87 44 1553 0 10 77.563258 2 id026 id089 id0000008985 39 0 5118 1 8 67.640243 3 id010 id002 id0000005883 49 19 1774 4 14 58.275440 4 id092 id052 id0000003463 59 83 9388 3 13 86.269280 In [ 4 ]:","title":"step 0a: get the data"},{"location":"week2.4.html#step-0b-create-a-timing-function","text":"As we are creating a benchmark in this exercise, we are going to perform the same operation on the dataframe (either Dask or Pandas) multiple times, in which we are only interested in the time it took to perform that operation. So, it makes sense to create a function that receives a dataframe, performs the operation and returns the time it took to finish. Create a function perform_test() that does exactly this. During this benchmark, we are going to group the dataframe on id1 and sum up the values of v1 . Note: since this function needs to be called with both a pandas dataframe and a dask dataframe, you need to take into account that the last one uses lazy evaluation; so if the dataframe is from dask, you need to call compute() in the end. Use isinstance to check which type of dataframe you are receiving.","title":"step 0b: create a timing function"},{"location":"week2.4.html#exercise-1-improve-on-the-dask-dataframe","text":"","title":"Exercise 1: improve on the Dask dataframe"},{"location":"week2.4.html#step-1-setting-a-dask-baseline","text":"Let\u2019s run the groupby query with Dask. We\u2019re going to intentionally type all the columns except v1 as object columns, which should give us a good worst case Dask performance benchmark. object columns are notoriously memory hungry and inefficient. Use dask.read_csv with the following dictionary for the ntype -parameter to load our test-dataset and change all the columns into obect s. dtypes = { \"id1\" : \"object\" , \"id2\" : \"object\" , \"id3\" : \"object\" , \"id4\" : \"object\" , \"id5\" : \"object\" , \"id6\" : \"object\" , \"v1\" : \"int64\" , \"v2\" : \"object\" , \"v3\" : \"object\" , } As you no doubt will see, this computation takes far longer than our standard pandas implementation. Now let's see if we can speed this up a little bit.","title":"step 1: setting a dask baseline"},{"location":"week2.4.html#step-2-avoid-object-columns","text":"The datatype object is notoriously inefficient when it comes to computation and calculation. If we know the data-type of a column, it is always better to provide exactly this. We can type id1 , id2 , and id3 as string[pyarrow] type columns, which are way more efficient. The column v3 seems to be a float64 and the rest of the columns can be seen as int64 . Change the parameter dtype to reflect these improvements and rerun the test. Do you see an increase in performance?","title":"step 2: avoid object columns"},{"location":"week2.4.html#step-3-using-multiple-files","text":"Dask can read and write multiple files in parallel. Since parallel I/O is normally a lot faster than working with a single large file, it can make sense to split the data over multiple files. Use the ddf.repartition() method with a partition_size of '100MB' to create several different files. Look at the test -directory (or whereever you have stored the files) in order to see the files this method has created. Now, use read_csv with a wildcard to create a dask dataframe with all these newly created files (don't forget to include the correct datatype from the previous exercise). Next, run the perform_test again.","title":"step 3: using multiple files"},{"location":"week2.4.html#step-4a-parquet-instead-of-csv","text":"CSV is actually a bad data format. It is, as we say, row based , so it is impossible to drop certain columns while reading in the data. Columnar file formats that are stored as binary usually perform better than row-based, text file formats like CSV. Compressing the files to create smaller file sizes also helps. Read more about dast dataframes and Parquet on this documentation site . Use dask.to_parquet in the same manner as you have created the several csv-files in the previous exercise. For now, use compression=None as a parameter. Also, use pyarrow as an engine (you might need to pip install this). Next, use dask.read_parquet to read in the parquet-files (remember to use the same engine you used when creating the files) and perform the test again.","title":"step 4a: parquet instead of csv"},{"location":"week2.4.html#step-4b-use-snappy-as-a-compressor","text":"Recreate the parquet-files using snappy (which you also probably need to pip install : be sure to install python-snappy ) as an engine and reperform the test. Is there an increase in the efficiency?","title":"step 4b: use snappy as a compressor"},{"location":"week2.4.html#step-5-column-pruning","text":"In contrast to csv, parquet is a columnar file format, which means you can selectively grab certain columns from the file. This is commonly referred to as column pruning . Column pruning isn\u2019t possible for row based file formats like CSV. Have a look at the documentation for dask.read_csv to see how you can select only the columns that are relevant for our query: id1 and v1 . Create a new dataframe from our parquet-files with only these columns and, again, perform the test.","title":"step 5: column pruning"},{"location":"week2.4.html#step-6-comparison-with-pandas","text":"Now, after all this work, we need to see whether dask is actually performing better than pandas. Create a dataframe on basis of the csv-file and run our perform_test -function. Note the time it takes to execute; is this better than, just as good as, or worse than dask...? You will probably see that pandas performs way better than dask, even with the last improvements on the data. So why go through all the hassle? You will see that when you redo the exercise with ten times as much data as the dataset that we had. This means that you have to take not only the computation you are doing into account when thinking about Dask, but also about the size of your dataset. It only makes sense to use Dask when your dataset is really large.","title":"step 6: comparison with pandas"},{"location":"week2.4.html#exercise-2-lazy-evaluation","text":"During the planery part, we have talked about the possibility of lazy evaluation and parallel computing. Reformat your test code so that you make use of this techniques for the computation \u2013 see the relevant code below. Do you see an increase in the performance of Dask? import dask lazy_results = [] for parameters in input_params . values : lazy_result = dask . delayed ( costly_simulation )( parameters ) lazy_results . append ( lazy_result ) futures = dask . persist ( * lazy_results ) You can even try to increase the number of clusters on your local machine, using client.cluster.scale() , as was shown in the plenary part.","title":"Exercise 2: Lazy evaluation"},{"location":"week2.5.html","text":"Week 2.5: Real world Dask \u00b6 Introduction \u00b6 The goal of this assignment is to read in a large dataset of protein annotation information and to manipulate, summarize and analyze it using Dask Dataframes. Protein annotation is a branch of bioinformatics which classifies the different parts of a protein's structure based on both sequence and functional characteristics. For instance, it recognizes structural elements like trans-membrane helices, but also particular active sites (\"Serine Protease\") and also signal peptides (\"periplasmic membrane tag\"). The holy grail of this field is to use these different annotations of parts of the protein sequence, and to combine them to predict the function of the protein as a whole, without having to carry out actual experiments in the lab. The subject is the output of the InterProScan protein annotation service InterproScan online , NAR article . Briefly, InterPROscan is a meta-annotator: it runs different protein function annotators in turn on an input amino-acid sequence FASTA file and collects the output of each, labelling them with a unique and consistent identifier \u2013 the \"InterPRO number\". This service is used to annotate all currently known prokaryotic (Bacteria, Archaea) genomes to investigate better methods of metagenomics sequence annotation. 2. Deliverables \u00b6 Write a script that reads in a InterPROscan output file and answers the questions below. You can test your script on the data-file that you can find at /data/dataprocessing/interproscan/all_bacilli.tsv file on assemblix2012 and assemblix2019. You must use the Dask Dataframe interface to read in and manipulate this file. This file contains ~4,200,000 protein annotations. How many distinct protein annotations are found in the dataset? I.e. how many distinc InterPRO numbers are there? How many annotations does a protein have on average? What is the most common GO Term found? What is the average size of an InterPRO feature found in the dataset? What is the top 10 most common InterPRO features? If you select InterPRO features that are almost the same size (within 90-100%) as the protein itself, what is the top10 then? If you look at those features which also have textual annotation, what is the top 10 most common word found in that annotation? And the top 10 least common? Combining your answers for Q6 and Q7, what are the 10 most commons words found for the largest InterPRO features? What is the coefficient of correlation ( \\(R^2\\) ) between the size of the protein and the number of features found? Your script should output a CSV file with 3 columns: in the first column the question number in the second column the answer(s) to the question in the third column the output of the scheduler's physical plan (using the .explain() PySpark method) as a string NB1: Make sure you use the /commons/conda environment NB2: Do not download the file on your local machine, but work on it from the server. You can configure VSCode on your machine to connect (via ssh) to assemblix2019. NB3: Use only 16 threads maximum: dask.dataframe.compute(num_workers=16)","title":"Week 2.5: Real world Dask"},{"location":"week2.5.html#week-25-real-world-dask","text":"","title":"Week 2.5: Real world Dask"},{"location":"week2.5.html#introduction","text":"The goal of this assignment is to read in a large dataset of protein annotation information and to manipulate, summarize and analyze it using Dask Dataframes. Protein annotation is a branch of bioinformatics which classifies the different parts of a protein's structure based on both sequence and functional characteristics. For instance, it recognizes structural elements like trans-membrane helices, but also particular active sites (\"Serine Protease\") and also signal peptides (\"periplasmic membrane tag\"). The holy grail of this field is to use these different annotations of parts of the protein sequence, and to combine them to predict the function of the protein as a whole, without having to carry out actual experiments in the lab. The subject is the output of the InterProScan protein annotation service InterproScan online , NAR article . Briefly, InterPROscan is a meta-annotator: it runs different protein function annotators in turn on an input amino-acid sequence FASTA file and collects the output of each, labelling them with a unique and consistent identifier \u2013 the \"InterPRO number\". This service is used to annotate all currently known prokaryotic (Bacteria, Archaea) genomes to investigate better methods of metagenomics sequence annotation.","title":"Introduction"},{"location":"week2.5.html#2-deliverables","text":"Write a script that reads in a InterPROscan output file and answers the questions below. You can test your script on the data-file that you can find at /data/dataprocessing/interproscan/all_bacilli.tsv file on assemblix2012 and assemblix2019. You must use the Dask Dataframe interface to read in and manipulate this file. This file contains ~4,200,000 protein annotations. How many distinct protein annotations are found in the dataset? I.e. how many distinc InterPRO numbers are there? How many annotations does a protein have on average? What is the most common GO Term found? What is the average size of an InterPRO feature found in the dataset? What is the top 10 most common InterPRO features? If you select InterPRO features that are almost the same size (within 90-100%) as the protein itself, what is the top10 then? If you look at those features which also have textual annotation, what is the top 10 most common word found in that annotation? And the top 10 least common? Combining your answers for Q6 and Q7, what are the 10 most commons words found for the largest InterPRO features? What is the coefficient of correlation ( \\(R^2\\) ) between the size of the protein and the number of features found? Your script should output a CSV file with 3 columns: in the first column the question number in the second column the answer(s) to the question in the third column the output of the scheduler's physical plan (using the .explain() PySpark method) as a string NB1: Make sure you use the /commons/conda environment NB2: Do not download the file on your local machine, but work on it from the server. You can configure VSCode on your machine to connect (via ssh) to assemblix2019. NB3: Use only 16 threads maximum: dask.dataframe.compute(num_workers=16)","title":"2. Deliverables"},{"location":"week2.6.html","text":"Week 2.6: SE4ML \u00b6 Introduction \u00b6","title":"Week 2.6: SE4ML"},{"location":"week2.6.html#week-26-se4ml","text":"","title":"Week 2.6: SE4ML"},{"location":"week2.6.html#introduction","text":"","title":"Introduction"}]}