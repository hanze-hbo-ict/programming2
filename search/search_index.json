{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Programming 4 \u00b6 These pages provide the materials, information, and assignments for the module Programming 4 for the Master Digital Science for Life Science. In this module, we focus on the interesting subject software engineering for machine learning . We will provide some methods and techniques from the field of software engineering and see how we can apply those to data science (and, by extension, life sciences ). We delve in quite some depth into object life-cycle and multiple class interaction , generators and async programming , parallellisation , and design patterns . Specifically, the module contains the following subjects: weeknumber subject(s) 1 Introduction; SOLID; Static Code Analysis 2 OO: Classes and Objects; Constructors and Destructors; Object Lifecycle; Dunders 3 List Comprehensions; Generators; map-reduce; 4 Parallellisation and async programming 5 Putting it all together Assignment \u00b6 During the course, students will work individually on (more or less) weekly assignments. Every week, students are given time to work on these assignments and will give peer feedback on each other's work. Students can improve their elaboration on basis of this feedback. You can find the assignments (all five of them) in the menu on the left. Make a directory for each of the assignments ( assignment1/ , assignment2/ , you get the gist). At the end of the term, the collection of all these elaborations will form a portfolio which will be graded. Should the portfolio be considered insufficient, specific repair assignments will be given. github \u00b6 For this module, the use of github is mandatory. All work must be submitted to github in an orderly fashion \u2013 having a good and consistent git workflow is part of the requirements for the grade. This includes (but is not necessarily limited to) Using a local and remote version of your code base Branching and merging at specific points in your developmen cycle Writing good and informative commit messages Committing often No Notebook \u00b6 Even though we are really a big fan of Jupyter Notebook, in this module use of this tool is strictly forbidden. Writing software for the future means being able to write software that is readable, maintainable and re-usable, software that is ready for production. Thought there is some tooling that makes it possible to put notebooks in production, this is not what they are meant for. The use case for Jupyter Notebooks is exploration and research and in this module we are focussing on software development. A note on the use of AI \u00b6 We expect you to use AI (ChatGPT and image generation tools, at a minimum) in this module. Learning to use AI is an emerging skill and we will provide tutorials about how to do this in an efficient and interesting way. Be aware of the limits of ChatGPT: If you provide minimum effort prompts, you will get low quality results. You will need to refine your prompts in order to get good outcomes. This will take work and practice. Don't believe anything it says. If it gives you facts, assume them to be wrong and make sure you check them in some other way or reference. You will be responsible for any errors or omissions provided by the tool. AI is a tool, but one that you need to acknowledge using. Please include a paragraph at the end of any assignment that uses AI explaining what you used the AI for and what prompts you provided it with. Have a look at this page at apa.org to get an idea about how to reference to ChatGPT. Be thoughtful about when this tool is useful. Don't use it if it isn't appropriate for the case or for the circumstances.","title":"Introduction"},{"location":"index.html#programming-4","text":"These pages provide the materials, information, and assignments for the module Programming 4 for the Master Digital Science for Life Science. In this module, we focus on the interesting subject software engineering for machine learning . We will provide some methods and techniques from the field of software engineering and see how we can apply those to data science (and, by extension, life sciences ). We delve in quite some depth into object life-cycle and multiple class interaction , generators and async programming , parallellisation , and design patterns . Specifically, the module contains the following subjects: weeknumber subject(s) 1 Introduction; SOLID; Static Code Analysis 2 OO: Classes and Objects; Constructors and Destructors; Object Lifecycle; Dunders 3 List Comprehensions; Generators; map-reduce; 4 Parallellisation and async programming 5 Putting it all together","title":"Programming 4"},{"location":"index.html#assignment","text":"During the course, students will work individually on (more or less) weekly assignments. Every week, students are given time to work on these assignments and will give peer feedback on each other's work. Students can improve their elaboration on basis of this feedback. You can find the assignments (all five of them) in the menu on the left. Make a directory for each of the assignments ( assignment1/ , assignment2/ , you get the gist). At the end of the term, the collection of all these elaborations will form a portfolio which will be graded. Should the portfolio be considered insufficient, specific repair assignments will be given.","title":"Assignment"},{"location":"index.html#github","text":"For this module, the use of github is mandatory. All work must be submitted to github in an orderly fashion \u2013 having a good and consistent git workflow is part of the requirements for the grade. This includes (but is not necessarily limited to) Using a local and remote version of your code base Branching and merging at specific points in your developmen cycle Writing good and informative commit messages Committing often","title":"github"},{"location":"index.html#no-notebook","text":"Even though we are really a big fan of Jupyter Notebook, in this module use of this tool is strictly forbidden. Writing software for the future means being able to write software that is readable, maintainable and re-usable, software that is ready for production. Thought there is some tooling that makes it possible to put notebooks in production, this is not what they are meant for. The use case for Jupyter Notebooks is exploration and research and in this module we are focussing on software development.","title":"No Notebook"},{"location":"index.html#a-note-on-the-use-of-ai","text":"We expect you to use AI (ChatGPT and image generation tools, at a minimum) in this module. Learning to use AI is an emerging skill and we will provide tutorials about how to do this in an efficient and interesting way. Be aware of the limits of ChatGPT: If you provide minimum effort prompts, you will get low quality results. You will need to refine your prompts in order to get good outcomes. This will take work and practice. Don't believe anything it says. If it gives you facts, assume them to be wrong and make sure you check them in some other way or reference. You will be responsible for any errors or omissions provided by the tool. AI is a tool, but one that you need to acknowledge using. Please include a paragraph at the end of any assignment that uses AI explaining what you used the AI for and what prompts you provided it with. Have a look at this page at apa.org to get an idea about how to reference to ChatGPT. Be thoughtful about when this tool is useful. Don't use it if it isn't appropriate for the case or for the circumstances.","title":"A note on the use of AI"},{"location":"assignment1.html","text":"Assignment 1: Static code analysis \u00b6 During the plenary part, we discussed the necessity of static code analysis. We looked at the ontology of Python-applications, going from functions to classes to modules to systems. We also had a look at more high-level descriptions of code, using the c4-model to describe any software system. For this assignment, you are required to write a small report of your findings on one of the code bases listed below . This report should be written in a file report.md in the corresponding directory (i.e. assigment1/ ). If you want to include images (which should be considered good practice), please put them in the directory assignment1/imgs . We suggest plantuml to create technical images. Report \u00b6 Start your analysis with a short discription of the code-base: what it does, its intended use-case and public (basically a summary of the project's readme -file). Also give a description of its status on github (number of PR's, issues, pulse etc.). After that, give a high-level overview of the code-base: how is the code organized, what is the architectural setup, dependencies, build strategies etc. Images such as class- or sequence-diagrams are helpful at this level. Next, give a short quantitative description of the code-base: how many LoC, how many functions/methods/classes, how many modules/packages/namespaces, etc. Also have a look at the issues and PR's on github to get an idea of how other people are experiencing the project. After this overview, you should look in some more depth to your code-base. You can make use of one of the technical tools that are suggested on c4model.com/tooling . Also, run at least one static analysis tool (e.g. pylint , flake8 , sonarqube , bandit , cloc , or a language-specific alternative). Give attention to at least the following items: Complexity metrics for each function/method Code smells Dead code, unused imports Security issues (if any) Conclude with a reflection about the project: what do you think of it, where do you see refactoring candidates and what would your advice be to the ones that made the it. The list \u00b6 Here is the list of projects that you can choose from. If you have another project you would like to analyse, please discuss this with the teacher. Name Language(s) Short desciption url tldr-pages Node/Python (and a lot of markdown) Community driven collection of man-page alternatives https://github.com/tldr-pages/tldr httpie Python CLI: human-friendly HTTP client for the API era https://github.com/httpie/cli glances Python Open-source system cross-platform monitoring tool https://github.com/nicolargo/glances poetry Python Basically a dependency manager manager https://github.com/python-poetry/poetry jq C Lightweight and flexible command-line JSON processor https://github.com/stedolan/jq micromark JavaScript Yet another markdown parser https://github.com/micromark/micromark fizzbuzz-enterprise-edition Java Example (parodie) of over-engineered enterprise code https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition thefuck Python Tool to corrects errors in previous console commands https://github.com/nvbn/thefuck jsonlint JavaScript Yet another json-linter https://github.com/zaach/jsonlint htop C Cross-platform interactive process viewer https://github.com/htop-dev/htop","title":"1. Static code analysis"},{"location":"assignment1.html#assignment-1-static-code-analysis","text":"During the plenary part, we discussed the necessity of static code analysis. We looked at the ontology of Python-applications, going from functions to classes to modules to systems. We also had a look at more high-level descriptions of code, using the c4-model to describe any software system. For this assignment, you are required to write a small report of your findings on one of the code bases listed below . This report should be written in a file report.md in the corresponding directory (i.e. assigment1/ ). If you want to include images (which should be considered good practice), please put them in the directory assignment1/imgs . We suggest plantuml to create technical images.","title":"Assignment 1: Static code analysis"},{"location":"assignment1.html#report","text":"Start your analysis with a short discription of the code-base: what it does, its intended use-case and public (basically a summary of the project's readme -file). Also give a description of its status on github (number of PR's, issues, pulse etc.). After that, give a high-level overview of the code-base: how is the code organized, what is the architectural setup, dependencies, build strategies etc. Images such as class- or sequence-diagrams are helpful at this level. Next, give a short quantitative description of the code-base: how many LoC, how many functions/methods/classes, how many modules/packages/namespaces, etc. Also have a look at the issues and PR's on github to get an idea of how other people are experiencing the project. After this overview, you should look in some more depth to your code-base. You can make use of one of the technical tools that are suggested on c4model.com/tooling . Also, run at least one static analysis tool (e.g. pylint , flake8 , sonarqube , bandit , cloc , or a language-specific alternative). Give attention to at least the following items: Complexity metrics for each function/method Code smells Dead code, unused imports Security issues (if any) Conclude with a reflection about the project: what do you think of it, where do you see refactoring candidates and what would your advice be to the ones that made the it.","title":"Report"},{"location":"assignment1.html#the-list","text":"Here is the list of projects that you can choose from. If you have another project you would like to analyse, please discuss this with the teacher. Name Language(s) Short desciption url tldr-pages Node/Python (and a lot of markdown) Community driven collection of man-page alternatives https://github.com/tldr-pages/tldr httpie Python CLI: human-friendly HTTP client for the API era https://github.com/httpie/cli glances Python Open-source system cross-platform monitoring tool https://github.com/nicolargo/glances poetry Python Basically a dependency manager manager https://github.com/python-poetry/poetry jq C Lightweight and flexible command-line JSON processor https://github.com/stedolan/jq micromark JavaScript Yet another markdown parser https://github.com/micromark/micromark fizzbuzz-enterprise-edition Java Example (parodie) of over-engineered enterprise code https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition thefuck Python Tool to corrects errors in previous console commands https://github.com/nvbn/thefuck jsonlint JavaScript Yet another json-linter https://github.com/zaach/jsonlint htop C Cross-platform interactive process viewer https://github.com/htop-dev/htop","title":"The list"},{"location":"assignment2.html","text":"Assignment 2: Classes and instances \u00b6 Introduction \u00b6 After the static code analysis, which checked the reading of code-bases, from now on we are going to focus on actually writing code. Photosynthesis is the process by which plants and certain algae convert light energy into chemical energy that can be stored and used to drive the organism's activities. Though different varieties of photosynthesis exist, the overall equation for the type that occurs in plants is as follows: \\(6CO_2 + 6H_2O -> C_6H_{12}O_6 + 6O_2 + energy\\) In this exercise, we are going to create a very simple model of this process. Assignment 1: the Atom class \u00b6 1a. Create a class Atom that is a representation of any atom in the periodic table. Make sure that when a concrete atom is instantiated, it is given its symbol, its atomic number and the number of neutrons in the core. Store those parameters in the created object. 1b. Create a method proton_number that returns the number of protons in the nucleus; make another method mass_number that returns the atom's mass number (the sum of protons and neutrons in the nucleus). Isotopes are types of atoms that have the same number of atomic number but a different number of neutrons in the core. So, e.g. 'normal' hydrogen has 1 proton and no neutrons in its nucleus, but it also comes in the form of deuterium whose nucleus contains a neutron (so it consists of 1 proton and 1 neutron) or even tritium (1 proton and 2 neutrons). 1c. Create a method isotope in the class Atom . When this method is called, the given number of neutrons must be replaced by whatever number is provided to this method (so this is an object mutating method ). 1d. We define an atom A to be less than another atom B if their proton number is the same (i.e. it is the same element) but the mass number of A is less than the mass number of B. Implement the methods that checks whether two isotopes of the same element are equal to each other, or less than or greater than each other. Raise an exception when the check is called with different types of elements. You can use the code below to test your implementation. protium = Atom ( 'H' , 1 , 0 ) deuterium = Atom ( 'H' , 1 , 1 ) oxygen = Atom ( 'O' , 8 , 8 ) tritium = Atom ( 'H' , 1 , 2 ) oxygen . isotope ( 9 ) assert tritium . neutrons == 2 assert tritium . mass_number () == 3 assert protium < deuterium assert deuterium <= tritium assert tritium >= protium print ( oxygen > tritium ) # <-- this should raise an Exception Assignment 2: the Molecule class \u00b6 A molecule is a neutral group of two or more atoms. 2a. Create the class Molecule . When creating an instance of this class, a list of tuples of two values (a pair ) is given. The first element of this pair is the Atom-object, and the second element is the number of atoms of that type that is put into the molecule. Thus, the following code snippet creates a water molecule: hydrogen = Atom ( 'H' , 1 , 1 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) 2b. Make sure that when we print individual molecules, we get something resembling the correct chemical formula (you don't have to take the exact protocol into account). So, e.g. print (water) would render H2O . Make sure that the number 1 is omitted in the representation. 2c In our small implementation, molecules that are created can never change (they are immutable ). However, we can add two molecules together in order to create a new molecule. Implement this method in the class Molecule . Creating molecules this way is, of course, not really possible. However, because of educational reasons, we pretend that this is an ok way to work. You can use the code below to test your implementation: hydrogen = Atom ( 'H' , 1 , 0 ) carbon = Atom ( 'C' , 6 , 6 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) print ( water ) # H2O print ( co2 ) # CO2 print ( water + co2 ) # H2OCO2 Assignment 3: The Chloroplast class \u00b6 As a final assignment, we are going to make a ( very, very ) simplified version of the photosynthesis process; basically, we are only going to implement the formula stated above. 3a. Create the class Chloroplast . When creating objects of this type, make sure two fields water and co2 are initialised at value 0 . 3b. Implement the following functionality: make a method add_molecule in which we can add water or carbon dioxide molecules. When we add either of them, the corresponding field is incremented by one. When we add something else than water or carbon dioxide, a ValueError is raised, but the program continues to function. If nothing else happens, this method returns an empty list 3c. When we have added a total of 6 CO2-molecules and 12 H2O-molecules, we start of the photosyntheses. We decrease the fields water and co2 with 6 and 12 respectively and create two new molecules: C6H12O6 and O2 (and energy, we we ignore that in this exercise). In this case, the method returns a list of tuples: 1 molecule of sugar and 6 molecules of oxygen (as per the general formula stated above). 3d. Make sure that when we print this instance of chloroplast, we get an idea of how many molecules of water and CO2 are already stored in it. You can use the following script to check your implementation water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) demo = Chloroplast () els = [ water , co2 ] while ( True ): print ( ' \\n What molecule would you like to add?' ) print ( '[1] Water' ) print ( '[2] carbondioxyde' ) print ( 'Please enter your choice: ' , end = '' ) try : choice = int ( input ()) res = demo . add_molecule ( els [ choice - 1 ]) if ( len ( res ) == 0 ): print ( demo ) else : print ( ' \\n === Photosynthesis!' ) print ( res ) print ( demo ) except Exception : print ( ' \\n === That is not a valid choice.' )","title":"2. Multiple Class Interaction"},{"location":"assignment2.html#assignment-2-classes-and-instances","text":"","title":"Assignment 2: Classes and instances"},{"location":"assignment2.html#introduction","text":"After the static code analysis, which checked the reading of code-bases, from now on we are going to focus on actually writing code. Photosynthesis is the process by which plants and certain algae convert light energy into chemical energy that can be stored and used to drive the organism's activities. Though different varieties of photosynthesis exist, the overall equation for the type that occurs in plants is as follows: \\(6CO_2 + 6H_2O -> C_6H_{12}O_6 + 6O_2 + energy\\) In this exercise, we are going to create a very simple model of this process.","title":"Introduction"},{"location":"assignment2.html#assignment-1-the-atom-class","text":"1a. Create a class Atom that is a representation of any atom in the periodic table. Make sure that when a concrete atom is instantiated, it is given its symbol, its atomic number and the number of neutrons in the core. Store those parameters in the created object. 1b. Create a method proton_number that returns the number of protons in the nucleus; make another method mass_number that returns the atom's mass number (the sum of protons and neutrons in the nucleus). Isotopes are types of atoms that have the same number of atomic number but a different number of neutrons in the core. So, e.g. 'normal' hydrogen has 1 proton and no neutrons in its nucleus, but it also comes in the form of deuterium whose nucleus contains a neutron (so it consists of 1 proton and 1 neutron) or even tritium (1 proton and 2 neutrons). 1c. Create a method isotope in the class Atom . When this method is called, the given number of neutrons must be replaced by whatever number is provided to this method (so this is an object mutating method ). 1d. We define an atom A to be less than another atom B if their proton number is the same (i.e. it is the same element) but the mass number of A is less than the mass number of B. Implement the methods that checks whether two isotopes of the same element are equal to each other, or less than or greater than each other. Raise an exception when the check is called with different types of elements. You can use the code below to test your implementation. protium = Atom ( 'H' , 1 , 0 ) deuterium = Atom ( 'H' , 1 , 1 ) oxygen = Atom ( 'O' , 8 , 8 ) tritium = Atom ( 'H' , 1 , 2 ) oxygen . isotope ( 9 ) assert tritium . neutrons == 2 assert tritium . mass_number () == 3 assert protium < deuterium assert deuterium <= tritium assert tritium >= protium print ( oxygen > tritium ) # <-- this should raise an Exception","title":"Assignment 1: the Atom class"},{"location":"assignment2.html#assignment-2-the-molecule-class","text":"A molecule is a neutral group of two or more atoms. 2a. Create the class Molecule . When creating an instance of this class, a list of tuples of two values (a pair ) is given. The first element of this pair is the Atom-object, and the second element is the number of atoms of that type that is put into the molecule. Thus, the following code snippet creates a water molecule: hydrogen = Atom ( 'H' , 1 , 1 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) 2b. Make sure that when we print individual molecules, we get something resembling the correct chemical formula (you don't have to take the exact protocol into account). So, e.g. print (water) would render H2O . Make sure that the number 1 is omitted in the representation. 2c In our small implementation, molecules that are created can never change (they are immutable ). However, we can add two molecules together in order to create a new molecule. Implement this method in the class Molecule . Creating molecules this way is, of course, not really possible. However, because of educational reasons, we pretend that this is an ok way to work. You can use the code below to test your implementation: hydrogen = Atom ( 'H' , 1 , 0 ) carbon = Atom ( 'C' , 6 , 6 ) oxygen = Atom ( 'O' , 8 , 8 ) water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) print ( water ) # H2O print ( co2 ) # CO2 print ( water + co2 ) # H2OCO2","title":"Assignment 2: the Molecule class"},{"location":"assignment2.html#assignment-3-the-chloroplast-class","text":"As a final assignment, we are going to make a ( very, very ) simplified version of the photosynthesis process; basically, we are only going to implement the formula stated above. 3a. Create the class Chloroplast . When creating objects of this type, make sure two fields water and co2 are initialised at value 0 . 3b. Implement the following functionality: make a method add_molecule in which we can add water or carbon dioxide molecules. When we add either of them, the corresponding field is incremented by one. When we add something else than water or carbon dioxide, a ValueError is raised, but the program continues to function. If nothing else happens, this method returns an empty list 3c. When we have added a total of 6 CO2-molecules and 12 H2O-molecules, we start of the photosyntheses. We decrease the fields water and co2 with 6 and 12 respectively and create two new molecules: C6H12O6 and O2 (and energy, we we ignore that in this exercise). In this case, the method returns a list of tuples: 1 molecule of sugar and 6 molecules of oxygen (as per the general formula stated above). 3d. Make sure that when we print this instance of chloroplast, we get an idea of how many molecules of water and CO2 are already stored in it. You can use the following script to check your implementation water = Molecule ( [ ( hydrogen , 2 ), ( oxygen , 1 ) ] ) co2 = Molecule ( [ ( carbon , 1 ), ( oxygen , 2 ) ]) demo = Chloroplast () els = [ water , co2 ] while ( True ): print ( ' \\n What molecule would you like to add?' ) print ( '[1] Water' ) print ( '[2] carbondioxyde' ) print ( 'Please enter your choice: ' , end = '' ) try : choice = int ( input ()) res = demo . add_molecule ( els [ choice - 1 ]) if ( len ( res ) == 0 ): print ( demo ) else : print ( ' \\n === Photosynthesis!' ) print ( res ) print ( demo ) except Exception : print ( ' \\n === That is not a valid choice.' )","title":"Assignment 3: The Chloroplast class"},{"location":"assignment3.html","text":"Assignment 3: List comprehensions and generators \u00b6 Motif Discovery via Custom Python Module with CLI Support \u00b6 Motif discovery, such as identifying overrepresented k-mers , is a core technique in genomic analysis. Your goal is to build a robust command-line tool that researchers can use to process DNA sequences, identify top motifs, and optionally filter them by GC content. Your code should eventually consist of the following files. The requirements are described sequentially below. motiftools.py \u2013 your reusable module motifcli.py \u2013 a command-line interface script Example data \u2013 a FASTA file or sequence list in .txt or .csv README.md \u2013 documentation, usage examples, and development notes 1. motiftools.py : Module Functions This file contains your module functions: kmer_generator(seq: str, k: int) -> Generator[str, None, None] : yields all overlapping k-mers from the sequence. Example: kmer_generator(\"ATGCG\", 3) yields \"ATG\" , \"TGC\" , \"GCG\" count_kmers(sequences: list[str], k: int) -> dict[str, int] : counts all k-mers across the input sequences. find_top_kmers(kmer_counts: dict[str, int], top_n: int = 10) -> list[tuple[str, int]] : returns the most frequent k-mers sorted by frequency. gc_content(seq: str) -> float : computes GC percentage of a sequence (use generator expressions). filter_kmers_by_gc(kmer_counts: dict[str, int], min_gc: float) -> list[str] : returns only those k-mers with GC content above the threshold. Use generator expressions , list comprehensions , and good function design. 2. motifcli.py : Command-Line Interface This script should enable researchers to use your program using a nice comman line interface. It should have the following command line options: --input (str): Path to a .txt file with one sequence per line (or FASTA) --k (int): Length of the motif (k-mer) --top (int): Number of top motifs to display --min-gc (float, optional): Minimum GC content to filter motifs python motifcli.py --input sequences.txt --k 5 --top 8 --min-gc 50 Should return something like: Top 8 motifs ( k = 5 ) : ATGCG - 15 GCGAT - 13 ... Filtered by GC content > 50 %: ATGCG GCGAT ... Implement this module using argparse .","title":"3. List comprehensions and generators"},{"location":"assignment3.html#assignment-3-list-comprehensions-and-generators","text":"","title":"Assignment 3: List comprehensions and generators"},{"location":"assignment3.html#motif-discovery-via-custom-python-module-with-cli-support","text":"Motif discovery, such as identifying overrepresented k-mers , is a core technique in genomic analysis. Your goal is to build a robust command-line tool that researchers can use to process DNA sequences, identify top motifs, and optionally filter them by GC content. Your code should eventually consist of the following files. The requirements are described sequentially below. motiftools.py \u2013 your reusable module motifcli.py \u2013 a command-line interface script Example data \u2013 a FASTA file or sequence list in .txt or .csv README.md \u2013 documentation, usage examples, and development notes 1. motiftools.py : Module Functions This file contains your module functions: kmer_generator(seq: str, k: int) -> Generator[str, None, None] : yields all overlapping k-mers from the sequence. Example: kmer_generator(\"ATGCG\", 3) yields \"ATG\" , \"TGC\" , \"GCG\" count_kmers(sequences: list[str], k: int) -> dict[str, int] : counts all k-mers across the input sequences. find_top_kmers(kmer_counts: dict[str, int], top_n: int = 10) -> list[tuple[str, int]] : returns the most frequent k-mers sorted by frequency. gc_content(seq: str) -> float : computes GC percentage of a sequence (use generator expressions). filter_kmers_by_gc(kmer_counts: dict[str, int], min_gc: float) -> list[str] : returns only those k-mers with GC content above the threshold. Use generator expressions , list comprehensions , and good function design. 2. motifcli.py : Command-Line Interface This script should enable researchers to use your program using a nice comman line interface. It should have the following command line options: --input (str): Path to a .txt file with one sequence per line (or FASTA) --k (int): Length of the motif (k-mer) --top (int): Number of top motifs to display --min-gc (float, optional): Minimum GC content to filter motifs python motifcli.py --input sequences.txt --k 5 --top 8 --min-gc 50 Should return something like: Top 8 motifs ( k = 5 ) : ATGCG - 15 GCGAT - 13 ... Filtered by GC content > 50 %: ATGCG GCGAT ... Implement this module using argparse .","title":"Motif Discovery via Custom Python Module with CLI Support"},{"location":"assignment4.html","text":"Assignment 4: Parallellisation \u00b6 Introduction \u00b6 In the plenairy part we have discussed how you can make use of Python's multiprocessor module to distribute the computation of your problem over multiple cores on your CPU. We have seen how this increased performances when you are working with a lot of data, or when you need to do complex calculations. Even though the multiprocessor module is quite complex, the basic workings of it is quite simple. You provide your code with the number of cores you wish to use, some data you want to work with and a function that needs to be called for all the elements of your data. You use the function map for this minimal working example. For easy reference, the relevant portion of the code of the plenairy part is repeated below: import multiprocessing as mp with mp . Pool () as p : res = p . map ( sum_squared , numbers ) In this exercise, we are going to determine the fluctuation of CG-pairs over the genome of an organism. To do this, we need to count the amount of CG in (overlapping or non-overlapping) 'windows' (e.g. 10_000bp). We do this both in a sequential and in a non-sequential way, keeping track of the time it takes in both cases. Assignment \u00b6 Download the genome of the E.coli . Write a Python-program that accepts a command-line argument -w, --window_size that represents the window-size \ud83e\udd2f with which the program flows over the genome, returning the percentage of CG in that window. Encapsulate this functionality in a class, that receives the window-size in its constructor. Make sure that when you print an instance of this class, the found percentages are printed. Also give the class the possibility to write the results to a csv-file. > python count.py -w 10_000 | head 00000 -10000: 0 .521 10000 -20000: 0 .499 20000 -30000: 0 .526 30000 -40000: 0 .532 40000 -50000: 0 .528 50000 -60000: 0 .516 60000 -70000: 0 .556 70000 -80000: 0 .536 80000 -90000: 0 .500 > Now create a second script that that creates several instances of this class with different values for the window-size (e.g. from 5_000 to 25_000). Do this both in a 'normal' way and with the use of the multiprocessor-module and determine the time each execution takes (you can use time.time() or make use of the module codetimer ). Make a small visualisation of the results.","title":"4. Parallellisation"},{"location":"assignment4.html#assignment-4-parallellisation","text":"","title":"Assignment 4: Parallellisation"},{"location":"assignment4.html#introduction","text":"In the plenairy part we have discussed how you can make use of Python's multiprocessor module to distribute the computation of your problem over multiple cores on your CPU. We have seen how this increased performances when you are working with a lot of data, or when you need to do complex calculations. Even though the multiprocessor module is quite complex, the basic workings of it is quite simple. You provide your code with the number of cores you wish to use, some data you want to work with and a function that needs to be called for all the elements of your data. You use the function map for this minimal working example. For easy reference, the relevant portion of the code of the plenairy part is repeated below: import multiprocessing as mp with mp . Pool () as p : res = p . map ( sum_squared , numbers ) In this exercise, we are going to determine the fluctuation of CG-pairs over the genome of an organism. To do this, we need to count the amount of CG in (overlapping or non-overlapping) 'windows' (e.g. 10_000bp). We do this both in a sequential and in a non-sequential way, keeping track of the time it takes in both cases.","title":"Introduction"},{"location":"assignment4.html#assignment","text":"Download the genome of the E.coli . Write a Python-program that accepts a command-line argument -w, --window_size that represents the window-size \ud83e\udd2f with which the program flows over the genome, returning the percentage of CG in that window. Encapsulate this functionality in a class, that receives the window-size in its constructor. Make sure that when you print an instance of this class, the found percentages are printed. Also give the class the possibility to write the results to a csv-file. > python count.py -w 10_000 | head 00000 -10000: 0 .521 10000 -20000: 0 .499 20000 -30000: 0 .526 30000 -40000: 0 .532 40000 -50000: 0 .528 50000 -60000: 0 .516 60000 -70000: 0 .556 70000 -80000: 0 .536 80000 -90000: 0 .500 > Now create a second script that that creates several instances of this class with different values for the window-size (e.g. from 5_000 to 25_000). Do this both in a 'normal' way and with the use of the multiprocessor-module and determine the time each execution takes (you can use time.time() or make use of the module codetimer ). Make a small visualisation of the results.","title":"Assignment"},{"location":"assignment5.html","text":"Assignment 5: A complete project \u00b6 Introduction \u00b6 In this last assignment, you are asked to work on a relative big application which you need to divide in several classes. You are going to develop a small pipeline with several interacting components that work independent of each other. During the development, you need to take into account all the things we talked about in this module. The application you are going to make, makes use of a trained machine learning model, so this assignment is kind of an integration of DS3 with Programming 2. Have a look at the general machine learning development cycle below. Though we will go through all the steps, the focus will be on steps 3, 4, 5 and 6. Step 1: getting and transforming the data \u00b6 For this exercise, we are elaborating on the anomaly detection study case that is part of the unsupervised part of DS3. You can find the dataset that is used at Kaggle: https://www.kaggle.com/datasets/nphantawee/pump-sensor-data Use your own dataset You can also use your own dataset, but only if it fulfills the following requirements: There is a logical (ontological, domain-specific) reason why this dataset grows over time, e.g. when more images become available every month, or new prices of something are published every day, or new network connections are delivered every week; The data you already have is large enough (contains enough instances) to be split along the lines described above, i.e. you must be able to train a model on roughly three-fifth of the dataset and test the complete pipeline at least twice on the remaining splits; In order for the model to work correctly, the data needs some kind of non trivial transformation , like removing outliers, or merging, dropping or calculating columns; Every run of the model on new data should result in an interesting plot of the data. Please check this with the teacher. The dataset contains sensor data from 1 April to 31 August 2018. We're going to train the model on the months April, May, and June and then use the trained model to predict the anomalies of the months July and August. Split the original dataset among these lines, so that eventually you have three files. We use the first of these to actually train the model, and then feed the second and third files to this persisted model sequentially. Step 2: create the model and the drawer \u00b6 Using the first of the three splits described above, train one of the models from the notebook, or use your own realisation. Note that in order to do this training, some data-transformations are necessary. Isolate these relevant transformations, so that they can be performed later on the two remaining sets. Train the model on the transformed dataset. Use one of the techniques described on scikit-learn.org to persist the model on your local file system. Transform only the training-data For this exercise, it is imparitive that you perform the data-transformations after you split the original data and only on the training-data. In this way, we can correctly simulate the entering of future data in our pipeline. Study the method plot_sensor_anomolies(sensor, name) that you can find in the notebook of DS3 . In the current setup, this method can only work if the dataframe df is defined within the scope of the complete notebook (so only if df is a global variable ). You can use this method, but you need to refactor it, so that is independent of any global variables (global variables are bad news in general). Also, change its functionality so that it returns the plot instead of displaying it. Step 4: listening for new data \u00b6 Now that we have trained and saved our model, it is time to put it into production. For this to work, we are going to make another class that looks at a specific directory. When a new data file is uploaded to this directory, this class will load the data in that file, apply the necessary changes and use our model to predict new new values. It then saves the transformed data with the predictions in another directory and removes the original data file. Apart from saving the transformed and enriched data, it will also create images of certain sensors and save those in the img directory \u2013 their filename displaying both the sensor and the timestamp. Have a look at the sequence diagram below to get an idea of what is happening. The whole process need to be logged in a log-file. A typical run will produce the following lines in this log-file: 2024 -06-11 10 :09:32 Found new data file 2024 -06-11 10 :09:32 Loaded the file 2024 -06-11 10 :09:34 Received transformed data 2024 -06-11 10 :09:38 Received predicions 2024 -06-11 10 :09:38 Saving predictions 2024 -06-11 10 :09:40 Saving image2 2018 -07-sensor04.png 2024 -06-11 10 :09:40 Saving image2 2018 -07-sensor51.png 2024 -06-11 10 :09:40 Resuming listening If the new file in the input -directory does not contain data that is eligable for the system, an error needs to be logged to the log-file. This should, however, not break the running of the application. Test your realisation with the two separate data files you have created \u2013 the ones containing the data for July and for August: when you put one of them in the input -directory, the whole cycle should start and create a new data file in the output -directory and a new image in the img -directory. Technical requirements all files and classes should be independent of each other: they should adhere to the SOLID-principles. the application should make use of a file application.json in which the following settings are present: the location of the input -directory the location of the output -directory the location of the img -directory the names of the sensors that needs to be drawn the interval (in seconds) with which the input -directory is checked for new files","title":"5. Final assignment"},{"location":"assignment5.html#assignment-5-a-complete-project","text":"","title":"Assignment 5: A complete project"},{"location":"assignment5.html#introduction","text":"In this last assignment, you are asked to work on a relative big application which you need to divide in several classes. You are going to develop a small pipeline with several interacting components that work independent of each other. During the development, you need to take into account all the things we talked about in this module. The application you are going to make, makes use of a trained machine learning model, so this assignment is kind of an integration of DS3 with Programming 2. Have a look at the general machine learning development cycle below. Though we will go through all the steps, the focus will be on steps 3, 4, 5 and 6.","title":"Introduction"},{"location":"assignment5.html#step-1-getting-and-transforming-the-data","text":"For this exercise, we are elaborating on the anomaly detection study case that is part of the unsupervised part of DS3. You can find the dataset that is used at Kaggle: https://www.kaggle.com/datasets/nphantawee/pump-sensor-data Use your own dataset You can also use your own dataset, but only if it fulfills the following requirements: There is a logical (ontological, domain-specific) reason why this dataset grows over time, e.g. when more images become available every month, or new prices of something are published every day, or new network connections are delivered every week; The data you already have is large enough (contains enough instances) to be split along the lines described above, i.e. you must be able to train a model on roughly three-fifth of the dataset and test the complete pipeline at least twice on the remaining splits; In order for the model to work correctly, the data needs some kind of non trivial transformation , like removing outliers, or merging, dropping or calculating columns; Every run of the model on new data should result in an interesting plot of the data. Please check this with the teacher. The dataset contains sensor data from 1 April to 31 August 2018. We're going to train the model on the months April, May, and June and then use the trained model to predict the anomalies of the months July and August. Split the original dataset among these lines, so that eventually you have three files. We use the first of these to actually train the model, and then feed the second and third files to this persisted model sequentially.","title":"Step 1: getting and transforming the data"},{"location":"assignment5.html#step-2-create-the-model-and-the-drawer","text":"Using the first of the three splits described above, train one of the models from the notebook, or use your own realisation. Note that in order to do this training, some data-transformations are necessary. Isolate these relevant transformations, so that they can be performed later on the two remaining sets. Train the model on the transformed dataset. Use one of the techniques described on scikit-learn.org to persist the model on your local file system. Transform only the training-data For this exercise, it is imparitive that you perform the data-transformations after you split the original data and only on the training-data. In this way, we can correctly simulate the entering of future data in our pipeline. Study the method plot_sensor_anomolies(sensor, name) that you can find in the notebook of DS3 . In the current setup, this method can only work if the dataframe df is defined within the scope of the complete notebook (so only if df is a global variable ). You can use this method, but you need to refactor it, so that is independent of any global variables (global variables are bad news in general). Also, change its functionality so that it returns the plot instead of displaying it.","title":"Step 2: create the model and the drawer"},{"location":"assignment5.html#step-4-listening-for-new-data","text":"Now that we have trained and saved our model, it is time to put it into production. For this to work, we are going to make another class that looks at a specific directory. When a new data file is uploaded to this directory, this class will load the data in that file, apply the necessary changes and use our model to predict new new values. It then saves the transformed data with the predictions in another directory and removes the original data file. Apart from saving the transformed and enriched data, it will also create images of certain sensors and save those in the img directory \u2013 their filename displaying both the sensor and the timestamp. Have a look at the sequence diagram below to get an idea of what is happening. The whole process need to be logged in a log-file. A typical run will produce the following lines in this log-file: 2024 -06-11 10 :09:32 Found new data file 2024 -06-11 10 :09:32 Loaded the file 2024 -06-11 10 :09:34 Received transformed data 2024 -06-11 10 :09:38 Received predicions 2024 -06-11 10 :09:38 Saving predictions 2024 -06-11 10 :09:40 Saving image2 2018 -07-sensor04.png 2024 -06-11 10 :09:40 Saving image2 2018 -07-sensor51.png 2024 -06-11 10 :09:40 Resuming listening If the new file in the input -directory does not contain data that is eligable for the system, an error needs to be logged to the log-file. This should, however, not break the running of the application. Test your realisation with the two separate data files you have created \u2013 the ones containing the data for July and for August: when you put one of them in the input -directory, the whole cycle should start and create a new data file in the output -directory and a new image in the img -directory. Technical requirements all files and classes should be independent of each other: they should adhere to the SOLID-principles. the application should make use of a file application.json in which the following settings are present: the location of the input -directory the location of the output -directory the location of the img -directory the names of the sensors that needs to be drawn the interval (in seconds) with which the input -directory is checked for new files","title":"Step 4: listening for new data"},{"location":"files/Study_Case_Anomaly_Detection.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Anomaly detection algorithms \u00b6 Unsupervised anomaly detection algorithms can detect data that does not belong to normal data patterns. In general, it looks for isolated samples, samples that are in low-density regions. Anomaly detection algorithms can be divided into several approaches such as: Density based Distance based Kernel based Statistical approaches Tree-based methods In this notebook we test a few of these algorithms. The Local Outlier Factor, robust covariance, One Class Support Vector Machine, and the Isolation Forest. Local Outlier Factor LOF compares the density of any given data point to the density of its neighbors. Since outliers come from low-density areas, the ratio will be higher for anomalous data points. As a rule of thumb, a normal data point has a LOF between 1 and 1.5 whereas anomalous observations will have much higher LOF. The higher the LOF the more likely it is an outlier (2). Density-based methods identify anomalies based on the local density of data points. Anomalies are detected as points that have significantly lower density compared to their neighboring points. Another example next to LOF is DBSCAN. Distance-based methods determine anomalies by measuring the distance or dissimilarity of data points to their neighboring points. Points that are significantly distant or dissimilar from others are considered anomalies. Examples include k-nearest neighbors (k-NN), distance-based outlier detection (e.g., using Mahalanobis distance or Euclidean distance), and angle-based outlier detection. The robust covariance method is a gaussian elipse-based method . If it is outside the gaussian curve, or on the edges it might be an outlier. It is based on the Mahalanobis distance which assesses how many standard deviations \u03c3 away x\u1d62 is from \u03bc. An extreme observation has a large distance from the center of a distribution. An envelope around the data set can be constructed by choosing a critical value of the Mahalanobis distance. Points outside this envelope are considered anomalies/outliers. (1) One Class Support Vector Machine is cluster method based on the Support Vector Machine algorithm. Instead of separating classes into clusters, it separates one class from the origin. The points outside the boundary line are classified as outliers. It uses kernels. The default kernel is \u2018rbf\u2019 since most of the time the boundary is not linear. (3) Isolation Forest is a tree-based method. The Isolation Forest algorithm does not explicitly consider density or neighborhood relationships between data points. Instead, it uses the idea that anomalies can be identified more quickly and easily in the tree structure compared to normal data points. Anomalies are expected to have shorter paths in the tree, as they require fewer splits to be isolated. It uses partitioning to make isolated trees, with the goal that each point is isolated. The more partitioning is needed, the more chance a data point is a regular point. An outlier is much easier to isolate than a regular point. (4) [1] Nascimento et all. \"A cluster-based algorithm for anomaly detection in time series using Mahalanobis distance.\" Proceedings on the International Conference on Artificial Intelligence (ICAI). The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp), 2015. [2] Alghushairy, Omar, et al. \"A review of local outlier factor algorithms for outlier detection in big data streams.\" Big Data and Cognitive Computing 5.1 (2020): 1. [3] Sch\u00f6lkopf, Bernhard, et al. \"Support vector method for novelty detection.\" Advances in neural information processing systems 12 (1999). [4] Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. \"Isolation-based anomaly detection.\" ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 1-39. Hypothesis \u00b6 Anomaly detection techniques are commonly employed in predictive maintenance strategies to identify abnormal patterns or behaviors in machine data that may indicate impending failures. By monitoring various sensor readings, operational parameters, or other relevant data from machines, anomaly detection algorithms can learn the normal behavior or expected patterns of the machines during their normal operation. When a deviation from the normal behavior is detected, it may indicate a potential failure or malfunction. The hypothesis is that the sensor readings of a pump will generate not normal values in case of an (upcoming) failure, and these can be determined with anomaly detection. Several algorithms will be used to evaluate the hypothesis datasource: https://www.kaggle.com/datasets/nphantawee/pump-sensor-data Assignment \u00b6 Choose at least 1 out of the listed assignments Analyze the end result plot to evaluate the algorithm's performance. Look for anomalies identified by the algorithm and compare them to known anomalies or instances of abnormal behavior in the data. Assess whether the algorithm successfully captures these anomalies and if it shows promising results in detecting abnormal patterns. Based on the plot analysis, provide argumentation for the validity of the anomaly detection algorithm hypothesis (see above). Discuss how the algorithm effectively captures anomalies in the time series data and why it is a suitable approach for the use case. Support your argument with references to relevant literature that discuss the effectiveness of the chosen algorithm or similar algorithms in detecting anomalies in time series data. To improve data quality for anomaly detection in time series data, we can focus on removing outliers due to sensor reading errors while preserving anomalies. Additionally, performing resampling or aggregation can help reduce noise and highlight higher-level patterns relevant for anomaly detection. Find common steps to improve data quality for this particular use and implement those. Evaluate the performance of the algorithms compare to the original notebook. Try another algorithm that has not been tested in the notebook. Provide argumentation for the validity of the anomaly detection algorithm you choose. Try to find the best configuration for one of the tested algorithms Try anomaly detection on a different dataset without timeseries data. Evaluate the outcome import pandas as pd import matplotlib.pyplot as plt The data \u00b6 # source: https://www.kaggle.com/datasets/nphantawee/pump-sensor-data df = pd . read_csv ( 'data/sensor.csv' ) . drop ( 'Unnamed: 0' , axis = 1 ) df_origin = df . copy () Inspect data \u00b6 df . shape (220320, 54) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 220320 entries, 0 to 220319 Data columns (total 54 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 timestamp 220320 non-null object 1 sensor_00 210112 non-null float64 2 sensor_01 219951 non-null float64 3 sensor_02 220301 non-null float64 4 sensor_03 220301 non-null float64 5 sensor_04 220301 non-null float64 6 sensor_05 220301 non-null float64 7 sensor_06 215522 non-null float64 8 sensor_07 214869 non-null float64 9 sensor_08 215213 non-null float64 10 sensor_09 215725 non-null float64 11 sensor_10 220301 non-null float64 12 sensor_11 220301 non-null float64 13 sensor_12 220301 non-null float64 14 sensor_13 220301 non-null float64 15 sensor_14 220299 non-null float64 16 sensor_15 0 non-null float64 17 sensor_16 220289 non-null float64 18 sensor_17 220274 non-null float64 19 sensor_18 220274 non-null float64 20 sensor_19 220304 non-null float64 21 sensor_20 220304 non-null float64 22 sensor_21 220304 non-null float64 23 sensor_22 220279 non-null float64 24 sensor_23 220304 non-null float64 25 sensor_24 220304 non-null float64 26 sensor_25 220284 non-null float64 27 sensor_26 220300 non-null float64 28 sensor_27 220304 non-null float64 29 sensor_28 220304 non-null float64 30 sensor_29 220248 non-null float64 31 sensor_30 220059 non-null float64 32 sensor_31 220304 non-null float64 33 sensor_32 220252 non-null float64 34 sensor_33 220304 non-null float64 35 sensor_34 220304 non-null float64 36 sensor_35 220304 non-null float64 37 sensor_36 220304 non-null float64 38 sensor_37 220304 non-null float64 39 sensor_38 220293 non-null float64 40 sensor_39 220293 non-null float64 41 sensor_40 220293 non-null float64 42 sensor_41 220293 non-null float64 43 sensor_42 220293 non-null float64 44 sensor_43 220293 non-null float64 45 sensor_44 220293 non-null float64 46 sensor_45 220293 non-null float64 47 sensor_46 220293 non-null float64 48 sensor_47 220293 non-null float64 49 sensor_48 220293 non-null float64 50 sensor_49 220293 non-null float64 51 sensor_50 143303 non-null float64 52 sensor_51 204937 non-null float64 53 machine_status 220320 non-null object dtypes: float64(52), object(2) memory usage: 90.8+ MB df . machine_status . value_counts () NORMAL 205836 RECOVERING 14477 BROKEN 7 Name: machine_status, dtype: int64 #missing values percentage_missing = df . isnull () . sum () . sort_values ( ascending = False ) / len ( df ) * 100 percentage_missing . head () # show 5 largest missing % sensor_15 100.000000 sensor_50 34.956881 sensor_51 6.982117 sensor_00 4.633261 sensor_07 2.474129 dtype: float64 # drop low quality columns df . drop ([ 'sensor_15' , 'sensor_50' ], inplace = True , axis = 1 ) # set timestamp to date df [ 'timestamp' ] = pd . to_datetime ( df [ 'timestamp' ]) df = df . set_index ( 'timestamp' ) # This visualization inspired from JANANI KARIYAWASAM found at # https://www.kaggle.com/code/jananikariyawasam/data-cleaning-and-feature-engineering broken_rows = df [ df [ 'machine_status' ] == 'BROKEN' ] recovery_rows = df [ df [ 'machine_status' ] == 'RECOVERING' ] normal_rows = df [ df [ 'machine_status' ] == 'NORMAL' ] def plot_sensor ( sensor ): plot = plt . figure ( figsize = ( 25 , 3 )) plot = plt . plot ( recovery_rows [ sensor ], linestyle = 'none' , marker = 'o' , color = 'yellow' , markersize = 5 , label = 'recovering' ) plot = plt . plot ( df [ sensor ], color = 'grey' ) plot = plt . plot ( broken_rows [ sensor ], linestyle = 'none' , marker = 'X' , color = 'red' , markersize = 14 , label = 'broken' ) plot = plt . title ( sensor ) plot = plt . legend () plt . show (); # plot sensor 51 plot_sensor ( 'sensor_51' ) plot_sensor ( 'sensor_00' ) Preprocess for machine learning \u00b6 # use mean of the column to handle missing values and remove label in feature matrix X m , n = df . shape X = df . iloc [:,: n - 1 ] # ignore machine status columns X = X . fillna ( X . mean ()) X . shape (220320, 50) #scaling from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X = scaler . fit_transform ( X ) from sklearn import svm from sklearn.covariance import EllipticEnvelope from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor outliers_fraction = 1 - ( len ( normal_rows ) / ( len ( df ))) anomaly_algorithms = [ ( \"Isolation Forest\" , IsolationForest ( contamination = outliers_fraction , n_jobs = - 1 )), ( \"One-Class SVM\" , svm . OneClassSVM ( nu = outliers_fraction )), ( \"Local Outlier Factor\" , LocalOutlierFactor ( contamination = outliers_fraction , n_jobs = - 1 )), ( \"Robust covariance\" , EllipticEnvelope ( contamination = outliers_fraction ))] # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! # ## fit the models: Mind you this takes a lot of time!!!!!!!! # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! for name , algorithm in anomaly_algorithms : print ( algorithm ) if name == \"Local Outlier Factor\" : y_pred = algorithm . fit_predict ( X ) else : y_pred = algorithm . fit ( X ) . predict ( X ) df [ f ' { name } ' ] = y_pred print ( '-' * 100 ) print ( f 'number of anomolies detected' ) print ( df [ f ' { name } ' ] . value_counts ()) print ( '-' * 100 ) IsolationForest(contamination=0.06574074074074077, n_jobs=-1) ---------------------------------------------------------------------------------------------------- number of anomolies detected 1 205836 -1 14484 Name: Isolation Forest, dtype: int64 ---------------------------------------------------------------------------------------------------- OneClassSVM(nu=0.06574074074074077) ---------------------------------------------------------------------------------------------------- number of anomolies detected 1 205831 -1 14489 Name: One-Class SVM, dtype: int64 ---------------------------------------------------------------------------------------------------- LocalOutlierFactor(contamination=0.06574074074074077, n_jobs=-1) ---------------------------------------------------------------------------------------------------- number of anomolies detected 1 205836 -1 14484 Name: Local Outlier Factor, dtype: int64 ---------------------------------------------------------------------------------------------------- #save results filename = 'outcome.csv' df . to_csv ( filename , index = False ) # for name, algorithm in anomaly_algorithms: # anomoly_rows = df_results[df_results[f'{name}'] == -1] # print(anomoly_rows[['machine_status', f'{name}']]) # df = pd.read_csv('outcome.csv') machine_status Isolation Forest timestamp 2018-04-12 22:07:00 RECOVERING -1 2018-04-12 22:08:00 RECOVERING -1 2018-04-12 22:09:00 RECOVERING -1 2018-04-12 22:13:00 RECOVERING -1 2018-04-12 22:14:00 RECOVERING -1 ... ... ... 2018-08-23 05:32:00 NORMAL -1 2018-08-23 05:33:00 NORMAL -1 2018-08-23 05:36:00 NORMAL -1 2018-08-23 05:37:00 NORMAL -1 2018-08-23 05:38:00 NORMAL -1 [14484 rows x 2 columns] machine_status One-Class SVM timestamp 2018-04-02 14:04:00 NORMAL -1 2018-04-02 14:05:00 NORMAL -1 2018-04-02 14:06:00 NORMAL -1 2018-04-02 14:07:00 NORMAL -1 2018-04-02 14:08:00 NORMAL -1 ... ... ... 2018-08-31 23:09:00 NORMAL -1 2018-08-31 23:10:00 NORMAL -1 2018-08-31 23:11:00 NORMAL -1 2018-08-31 23:12:00 NORMAL -1 2018-08-31 23:13:00 NORMAL -1 [14489 rows x 2 columns] machine_status Local Outlier Factor timestamp 2018-04-01 04:02:00 NORMAL -1 2018-04-01 04:03:00 NORMAL -1 2018-04-01 04:35:00 NORMAL -1 2018-04-01 05:38:00 NORMAL -1 2018-04-01 05:43:00 NORMAL -1 ... ... ... 2018-08-31 21:34:00 NORMAL -1 2018-08-31 21:35:00 NORMAL -1 2018-08-31 21:36:00 NORMAL -1 2018-08-31 21:39:00 NORMAL -1 2018-08-31 22:11:00 NORMAL -1 [14484 rows x 2 columns] def plot_sensor_anomolies ( sensor , name ): anomoly_rows = df [ df [ f ' { name } ' ] == - 1 ] plot = plt . figure ( figsize = ( 25 , 3 )) plot = plt . plot ( df [ sensor ], color = 'grey' ) plot = plt . plot ( recovery_rows [ sensor ], linestyle = 'none' , marker = 'o' , color = 'yellow' , markersize = 5 , label = 'recovering' , alpha = 0.5 ) plot = plt . plot ( broken_rows [ sensor ], linestyle = 'none' , marker = 'X' , color = 'red' , markersize = 20 , label = 'broken' ) plot = plt . plot ( anomoly_rows [ sensor ], linestyle = 'none' , marker = 'X' , color = 'blue' , markersize = 4 , label = 'anomoly predicted' , alpha = 0.1 ) plot = plt . title ( sensor ) plot = plt . legend () plt . show (); plot_sensor_anomolies ( 'sensor_04' , 'Isolation Forest' ) Although the Isolation forest predicts anomolies near the broken points the outcome is not satisfactory.","title":"Study Case Anomaly Detection"},{"location":"files/Study_Case_Anomaly_Detection.html#anomaly-detection-algorithms","text":"Unsupervised anomaly detection algorithms can detect data that does not belong to normal data patterns. In general, it looks for isolated samples, samples that are in low-density regions. Anomaly detection algorithms can be divided into several approaches such as: Density based Distance based Kernel based Statistical approaches Tree-based methods In this notebook we test a few of these algorithms. The Local Outlier Factor, robust covariance, One Class Support Vector Machine, and the Isolation Forest. Local Outlier Factor LOF compares the density of any given data point to the density of its neighbors. Since outliers come from low-density areas, the ratio will be higher for anomalous data points. As a rule of thumb, a normal data point has a LOF between 1 and 1.5 whereas anomalous observations will have much higher LOF. The higher the LOF the more likely it is an outlier (2). Density-based methods identify anomalies based on the local density of data points. Anomalies are detected as points that have significantly lower density compared to their neighboring points. Another example next to LOF is DBSCAN. Distance-based methods determine anomalies by measuring the distance or dissimilarity of data points to their neighboring points. Points that are significantly distant or dissimilar from others are considered anomalies. Examples include k-nearest neighbors (k-NN), distance-based outlier detection (e.g., using Mahalanobis distance or Euclidean distance), and angle-based outlier detection. The robust covariance method is a gaussian elipse-based method . If it is outside the gaussian curve, or on the edges it might be an outlier. It is based on the Mahalanobis distance which assesses how many standard deviations \u03c3 away x\u1d62 is from \u03bc. An extreme observation has a large distance from the center of a distribution. An envelope around the data set can be constructed by choosing a critical value of the Mahalanobis distance. Points outside this envelope are considered anomalies/outliers. (1) One Class Support Vector Machine is cluster method based on the Support Vector Machine algorithm. Instead of separating classes into clusters, it separates one class from the origin. The points outside the boundary line are classified as outliers. It uses kernels. The default kernel is \u2018rbf\u2019 since most of the time the boundary is not linear. (3) Isolation Forest is a tree-based method. The Isolation Forest algorithm does not explicitly consider density or neighborhood relationships between data points. Instead, it uses the idea that anomalies can be identified more quickly and easily in the tree structure compared to normal data points. Anomalies are expected to have shorter paths in the tree, as they require fewer splits to be isolated. It uses partitioning to make isolated trees, with the goal that each point is isolated. The more partitioning is needed, the more chance a data point is a regular point. An outlier is much easier to isolate than a regular point. (4) [1] Nascimento et all. \"A cluster-based algorithm for anomaly detection in time series using Mahalanobis distance.\" Proceedings on the International Conference on Artificial Intelligence (ICAI). The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp), 2015. [2] Alghushairy, Omar, et al. \"A review of local outlier factor algorithms for outlier detection in big data streams.\" Big Data and Cognitive Computing 5.1 (2020): 1. [3] Sch\u00f6lkopf, Bernhard, et al. \"Support vector method for novelty detection.\" Advances in neural information processing systems 12 (1999). [4] Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. \"Isolation-based anomaly detection.\" ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 1-39.","title":"Anomaly detection algorithms"},{"location":"files/Study_Case_Anomaly_Detection.html#hypothesis","text":"Anomaly detection techniques are commonly employed in predictive maintenance strategies to identify abnormal patterns or behaviors in machine data that may indicate impending failures. By monitoring various sensor readings, operational parameters, or other relevant data from machines, anomaly detection algorithms can learn the normal behavior or expected patterns of the machines during their normal operation. When a deviation from the normal behavior is detected, it may indicate a potential failure or malfunction. The hypothesis is that the sensor readings of a pump will generate not normal values in case of an (upcoming) failure, and these can be determined with anomaly detection. Several algorithms will be used to evaluate the hypothesis datasource: https://www.kaggle.com/datasets/nphantawee/pump-sensor-data","title":"Hypothesis"},{"location":"files/Study_Case_Anomaly_Detection.html#assignment","text":"Choose at least 1 out of the listed assignments Analyze the end result plot to evaluate the algorithm's performance. Look for anomalies identified by the algorithm and compare them to known anomalies or instances of abnormal behavior in the data. Assess whether the algorithm successfully captures these anomalies and if it shows promising results in detecting abnormal patterns. Based on the plot analysis, provide argumentation for the validity of the anomaly detection algorithm hypothesis (see above). Discuss how the algorithm effectively captures anomalies in the time series data and why it is a suitable approach for the use case. Support your argument with references to relevant literature that discuss the effectiveness of the chosen algorithm or similar algorithms in detecting anomalies in time series data. To improve data quality for anomaly detection in time series data, we can focus on removing outliers due to sensor reading errors while preserving anomalies. Additionally, performing resampling or aggregation can help reduce noise and highlight higher-level patterns relevant for anomaly detection. Find common steps to improve data quality for this particular use and implement those. Evaluate the performance of the algorithms compare to the original notebook. Try another algorithm that has not been tested in the notebook. Provide argumentation for the validity of the anomaly detection algorithm you choose. Try to find the best configuration for one of the tested algorithms Try anomaly detection on a different dataset without timeseries data. Evaluate the outcome import pandas as pd import matplotlib.pyplot as plt","title":"Assignment"},{"location":"files/Study_Case_Anomaly_Detection.html#the-data","text":"# source: https://www.kaggle.com/datasets/nphantawee/pump-sensor-data df = pd . read_csv ( 'data/sensor.csv' ) . drop ( 'Unnamed: 0' , axis = 1 ) df_origin = df . copy ()","title":"The data"},{"location":"files/Study_Case_Anomaly_Detection.html#inspect-data","text":"df . shape (220320, 54) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 220320 entries, 0 to 220319 Data columns (total 54 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 timestamp 220320 non-null object 1 sensor_00 210112 non-null float64 2 sensor_01 219951 non-null float64 3 sensor_02 220301 non-null float64 4 sensor_03 220301 non-null float64 5 sensor_04 220301 non-null float64 6 sensor_05 220301 non-null float64 7 sensor_06 215522 non-null float64 8 sensor_07 214869 non-null float64 9 sensor_08 215213 non-null float64 10 sensor_09 215725 non-null float64 11 sensor_10 220301 non-null float64 12 sensor_11 220301 non-null float64 13 sensor_12 220301 non-null float64 14 sensor_13 220301 non-null float64 15 sensor_14 220299 non-null float64 16 sensor_15 0 non-null float64 17 sensor_16 220289 non-null float64 18 sensor_17 220274 non-null float64 19 sensor_18 220274 non-null float64 20 sensor_19 220304 non-null float64 21 sensor_20 220304 non-null float64 22 sensor_21 220304 non-null float64 23 sensor_22 220279 non-null float64 24 sensor_23 220304 non-null float64 25 sensor_24 220304 non-null float64 26 sensor_25 220284 non-null float64 27 sensor_26 220300 non-null float64 28 sensor_27 220304 non-null float64 29 sensor_28 220304 non-null float64 30 sensor_29 220248 non-null float64 31 sensor_30 220059 non-null float64 32 sensor_31 220304 non-null float64 33 sensor_32 220252 non-null float64 34 sensor_33 220304 non-null float64 35 sensor_34 220304 non-null float64 36 sensor_35 220304 non-null float64 37 sensor_36 220304 non-null float64 38 sensor_37 220304 non-null float64 39 sensor_38 220293 non-null float64 40 sensor_39 220293 non-null float64 41 sensor_40 220293 non-null float64 42 sensor_41 220293 non-null float64 43 sensor_42 220293 non-null float64 44 sensor_43 220293 non-null float64 45 sensor_44 220293 non-null float64 46 sensor_45 220293 non-null float64 47 sensor_46 220293 non-null float64 48 sensor_47 220293 non-null float64 49 sensor_48 220293 non-null float64 50 sensor_49 220293 non-null float64 51 sensor_50 143303 non-null float64 52 sensor_51 204937 non-null float64 53 machine_status 220320 non-null object dtypes: float64(52), object(2) memory usage: 90.8+ MB df . machine_status . value_counts () NORMAL 205836 RECOVERING 14477 BROKEN 7 Name: machine_status, dtype: int64 #missing values percentage_missing = df . isnull () . sum () . sort_values ( ascending = False ) / len ( df ) * 100 percentage_missing . head () # show 5 largest missing % sensor_15 100.000000 sensor_50 34.956881 sensor_51 6.982117 sensor_00 4.633261 sensor_07 2.474129 dtype: float64 # drop low quality columns df . drop ([ 'sensor_15' , 'sensor_50' ], inplace = True , axis = 1 ) # set timestamp to date df [ 'timestamp' ] = pd . to_datetime ( df [ 'timestamp' ]) df = df . set_index ( 'timestamp' ) # This visualization inspired from JANANI KARIYAWASAM found at # https://www.kaggle.com/code/jananikariyawasam/data-cleaning-and-feature-engineering broken_rows = df [ df [ 'machine_status' ] == 'BROKEN' ] recovery_rows = df [ df [ 'machine_status' ] == 'RECOVERING' ] normal_rows = df [ df [ 'machine_status' ] == 'NORMAL' ] def plot_sensor ( sensor ): plot = plt . figure ( figsize = ( 25 , 3 )) plot = plt . plot ( recovery_rows [ sensor ], linestyle = 'none' , marker = 'o' , color = 'yellow' , markersize = 5 , label = 'recovering' ) plot = plt . plot ( df [ sensor ], color = 'grey' ) plot = plt . plot ( broken_rows [ sensor ], linestyle = 'none' , marker = 'X' , color = 'red' , markersize = 14 , label = 'broken' ) plot = plt . title ( sensor ) plot = plt . legend () plt . show (); # plot sensor 51 plot_sensor ( 'sensor_51' ) plot_sensor ( 'sensor_00' )","title":"Inspect data"},{"location":"files/Study_Case_Anomaly_Detection.html#preprocess-for-machine-learning","text":"# use mean of the column to handle missing values and remove label in feature matrix X m , n = df . shape X = df . iloc [:,: n - 1 ] # ignore machine status columns X = X . fillna ( X . mean ()) X . shape (220320, 50) #scaling from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X = scaler . fit_transform ( X ) from sklearn import svm from sklearn.covariance import EllipticEnvelope from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor outliers_fraction = 1 - ( len ( normal_rows ) / ( len ( df ))) anomaly_algorithms = [ ( \"Isolation Forest\" , IsolationForest ( contamination = outliers_fraction , n_jobs = - 1 )), ( \"One-Class SVM\" , svm . OneClassSVM ( nu = outliers_fraction )), ( \"Local Outlier Factor\" , LocalOutlierFactor ( contamination = outliers_fraction , n_jobs = - 1 )), ( \"Robust covariance\" , EllipticEnvelope ( contamination = outliers_fraction ))] # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! # ## fit the models: Mind you this takes a lot of time!!!!!!!! # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! for name , algorithm in anomaly_algorithms : print ( algorithm ) if name == \"Local Outlier Factor\" : y_pred = algorithm . fit_predict ( X ) else : y_pred = algorithm . fit ( X ) . predict ( X ) df [ f ' { name } ' ] = y_pred print ( '-' * 100 ) print ( f 'number of anomolies detected' ) print ( df [ f ' { name } ' ] . value_counts ()) print ( '-' * 100 ) IsolationForest(contamination=0.06574074074074077, n_jobs=-1) ---------------------------------------------------------------------------------------------------- number of anomolies detected 1 205836 -1 14484 Name: Isolation Forest, dtype: int64 ---------------------------------------------------------------------------------------------------- OneClassSVM(nu=0.06574074074074077) ---------------------------------------------------------------------------------------------------- number of anomolies detected 1 205831 -1 14489 Name: One-Class SVM, dtype: int64 ---------------------------------------------------------------------------------------------------- LocalOutlierFactor(contamination=0.06574074074074077, n_jobs=-1) ---------------------------------------------------------------------------------------------------- number of anomolies detected 1 205836 -1 14484 Name: Local Outlier Factor, dtype: int64 ---------------------------------------------------------------------------------------------------- #save results filename = 'outcome.csv' df . to_csv ( filename , index = False ) # for name, algorithm in anomaly_algorithms: # anomoly_rows = df_results[df_results[f'{name}'] == -1] # print(anomoly_rows[['machine_status', f'{name}']]) # df = pd.read_csv('outcome.csv') machine_status Isolation Forest timestamp 2018-04-12 22:07:00 RECOVERING -1 2018-04-12 22:08:00 RECOVERING -1 2018-04-12 22:09:00 RECOVERING -1 2018-04-12 22:13:00 RECOVERING -1 2018-04-12 22:14:00 RECOVERING -1 ... ... ... 2018-08-23 05:32:00 NORMAL -1 2018-08-23 05:33:00 NORMAL -1 2018-08-23 05:36:00 NORMAL -1 2018-08-23 05:37:00 NORMAL -1 2018-08-23 05:38:00 NORMAL -1 [14484 rows x 2 columns] machine_status One-Class SVM timestamp 2018-04-02 14:04:00 NORMAL -1 2018-04-02 14:05:00 NORMAL -1 2018-04-02 14:06:00 NORMAL -1 2018-04-02 14:07:00 NORMAL -1 2018-04-02 14:08:00 NORMAL -1 ... ... ... 2018-08-31 23:09:00 NORMAL -1 2018-08-31 23:10:00 NORMAL -1 2018-08-31 23:11:00 NORMAL -1 2018-08-31 23:12:00 NORMAL -1 2018-08-31 23:13:00 NORMAL -1 [14489 rows x 2 columns] machine_status Local Outlier Factor timestamp 2018-04-01 04:02:00 NORMAL -1 2018-04-01 04:03:00 NORMAL -1 2018-04-01 04:35:00 NORMAL -1 2018-04-01 05:38:00 NORMAL -1 2018-04-01 05:43:00 NORMAL -1 ... ... ... 2018-08-31 21:34:00 NORMAL -1 2018-08-31 21:35:00 NORMAL -1 2018-08-31 21:36:00 NORMAL -1 2018-08-31 21:39:00 NORMAL -1 2018-08-31 22:11:00 NORMAL -1 [14484 rows x 2 columns] def plot_sensor_anomolies ( sensor , name ): anomoly_rows = df [ df [ f ' { name } ' ] == - 1 ] plot = plt . figure ( figsize = ( 25 , 3 )) plot = plt . plot ( df [ sensor ], color = 'grey' ) plot = plt . plot ( recovery_rows [ sensor ], linestyle = 'none' , marker = 'o' , color = 'yellow' , markersize = 5 , label = 'recovering' , alpha = 0.5 ) plot = plt . plot ( broken_rows [ sensor ], linestyle = 'none' , marker = 'X' , color = 'red' , markersize = 20 , label = 'broken' ) plot = plt . plot ( anomoly_rows [ sensor ], linestyle = 'none' , marker = 'X' , color = 'blue' , markersize = 4 , label = 'anomoly predicted' , alpha = 0.1 ) plot = plt . title ( sensor ) plot = plt . legend () plt . show (); plot_sensor_anomolies ( 'sensor_04' , 'Isolation Forest' ) Although the Isolation forest predicts anomolies near the broken points the outcome is not satisfactory.","title":"Preprocess for machine learning"}]}